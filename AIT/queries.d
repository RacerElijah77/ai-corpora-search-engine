{Q1 legal  applications} 
{Q2 free-text, information retrieval  applications} 
{Q3 evolutionary ideas} 
{Q4 connectionism} 
{Q5 neural network research that is biologically plausible} 
{Q6 reasoning about uncertainty} 
{Q7 educational applications} 
{Q8 genetic algorithms} 
{Q9 This thesis is a study of the computational complexity of machine
learning from examples in the distribution-free model introduced by L.
G. Valiant (V84). In the distribution-free model, a learning algorithm
receives positive and negative examples of an unknown target set (or
concept) that is chosen from some known class of sets (or concept
class). These examples are generated randomly according to a fixed but
unknown probability distribution representing Nature, and the goal of
the learning algorithm is to infer an hypothesis concept that closely
approximates the target concept with respect to the unknown
distribution. This thesis is concerned with proving theorems about
learning in this formal mathematical model.
	We are interested in the phenomenon of efficient learning in
the distribution-free model, in the standard polynomial-time sense.
Our results include general tools for determining the polynomial-time
learnability of a concept class, an extensive study of efficient
learning when errors are present in the examples, and lower bounds on
the number of examples required for learning in our model. A
centerpiece of the thesis is a series of results demonstrating the
computational difficulty of learning a number of well-studied concept
classes. These results are obtained by reducing some apparently hard
number-theoretic problems from cryptography to the learning problems.
The hard-to-learn concept classes include the sets represented by
Boolean formulae, deterministic finite automata and a simplified form
of neural networks. We also give algorithms for learning powerful
concept classes under the uniform distribution, and give equivalences
between natural models of efficient learnability.
	This thesis also includes detailed definitions and motivation
for the distribution-free model, a chapter discussing past research in
this model and related models, and a short list of important open
problems.} 
{Q10 In this research we will develop a framework for applying some
abstract heuristic search (AHS) methods to a well known NP-Complete
problem: the graph partitioning problem (GPP). The uniform graph
partitioning problem can be described as partitioning the nodes of a
graph into two sets of equal size to minimize the sum of the cost of
arcs having end-points in different sets. This problem has important
applications in VLSI design, computer compiler design, and in
placement and layout problems.
	Within this research we demonstrate that the solutions
obtained from using the traditional GPP heuristic are often of poor
quality. We introduce an extended local search procedure, which
performed extremely well for the test problems. A solution method
based on mathematical programming and Lagrangian relaxation is
introduced. The Lagrangian problem was used to (1) obtain good
feasible solutions, and (2) derive lower bounds for the graph
partitioning problem. The lower bounds facilitate a thorough empirical
analysis of the performance of the heuristic procedures. We further
introduce a new procedure for solving the GPP based on tabu search
(TS). This procedure includes a new technique for diversification in
TS. A new procedure for solving the GPP based on genetic algorithms
(GA) is also presented. This method includes a new technique for
selection, known as the "queen bee" strategy. We provide a common
ground for a thorough comparison of solution procedures for the GPP by
studying a worst-case measure of the various solutions' closeness to
optimality, and an average empirical worst-case measure for each
solution technique. This work constitutes the first empirical
comparison of simulated annealing, genetic algorithms and tabu search.
	The AHS techniques constitute a major step forward as general
problem solving techniques in that they overcome some of the
limitations of the traditional problem solving paradigms of operations
research and artificial intelligence. The AHS methods do not generally
take on strong assumptions regarding the shape of the feasible region,
nor regarding the form of the objective function. In addition, we
found that GA and TS are the superior techniques for solving the GPP,
both with respect to solution quality and computational requirements.}
