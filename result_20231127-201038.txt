#####################
* Score: 2463.9402890085184
* Doc #: 1898
* Title: BACK PROPAGATION: ACCELERATED LEARNING AND APPLICATION IN A TWO-DIMENSIONAL IMAGE ANALYSIS SYSTEM
* Autho: KOTHARI, RAVI
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; ARTIFICIALINTELLIGENCE
* Abstr: Back propagation has provided the elusive and much sought after
 mechanism of training multi-layered artificial neural networks. The
 substantial computational ability of such networks, coupled with the
 parallelism and generalization inherent in them, makes them an attractive
 proposition in the design of real time image analysis systems. However,the
 slow rate of training by back propagation poses to be a serious limitation
 particularly in cases where a network must continually adapt to new
 stimuli.
      This dissertation focuses on two aspects of neural network research.
 Phase I of this research is directed towards improving the back propagation
 algorithm to realize faster training of multi-layered networks. To this
 end, the concept of training with a gradual increase in accuracy is
 presented. It is shown that the proposed concept, on an average, doubles
 the rate of training of the back propagation algorithm, or the back
 propagation algorithm using a momentum weight change without increasing the
 size of the network or requiring additional support hardware.
      Phase II of this dissertation uses neural networks in the design of a
 real time 2-dimensional image analysis system. The system is proposed for
 use as part of an Automated Lumber Processing System (ALPS) which requires
 real time determination of the perimeter of a lumber board; the location,
 expanse and classification of surface defects on it. The system developed
 herein uses a multi-layered network to provide dynamic multiple thresholds
 for image segmentation. A manifestation attribute based classification
 scheme is then used to classify a defect into one of the five classes of
 defects commonly found on lumber. The high level of abstraction does not
 require computationally intensive statistical measures and allows an 8' x
 8" board to be processed in less than a minute using neural networks
 simulated in software on an i486-25MHz based computer. An overall accuracy
 of 96.57% was obtained in locating the surface defects that were present on
 the test samples, and an accuracy of 85.2% was obtained in classifying them
 into the five defect classes considered.
#####################
* Score: 1521.207297656219
* Doc #: 829
* Title: BAYESIAN EDGE-DETECTION IN IMAGE PROCESSING
* Autho: STEPHENS, DAVID A.
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Problems associated with the processing and statistical analysis of
 image data are the subject of much current interest, and many sophisticated
 techniques for extracting semantic content from degraded or corrupted
 images have been developed. However, such techniques often require
 considerable computational resources, and thus are, in certain
 applications, inappropriate. The detection localised discontinuities, or
 edges, in the image can be regarded as a pre-processing operation in
 relation to these sophisticated techniques which, if implemented
 efficiently and successfully, can provide a means for an exploratory
 analysis that is useful in two ways. First, such an analysis can be used to
 obtain quantitative information relating to the underlying structures from
 which the various regions in the image are derived about which we would
 generally be a priori ignorant. Secondly, in cases where the inference
 problem relates to discovery of the unknown location or dimensions of a
 particular region or object, or where we merely wish to infer the presence
 or absence of structures having a particular configuration, an accurate
 edge-detection analysis can circumvent the need for the subsequent
 sophisticated analysis. Relatively little interest has been focussed on the
 edge-detection problem within a statistical setting.
      In this thesis, we formulate the edge-detection problem in a formal
 statistical framework, and develop a simple and easily implemented
 technique for the analysis of images derived from two-region single edge
 scenes. We extend this technique in three ways; first, to allow the
 analysis of more complicated scenes, secondly, by incorporating spatial
 considerations, and thirdly, by considering images of various qualitative
 nature. We also study edge reconstruction and representation given the
 results obtained from the exploratory analysis, and a cognitive problem
 relating to the detection of objects modelled by members of a class of
 simple convex objects. Finally, we study in detail aspects of one of the
 sophisticated image analysis techniques, and the important general
 statistical applications of the theory on which it is founded.
#####################
* Score: 1521.207297656219
* Doc #: 4048
* Title: A CONTINUOUS DENSITY NEURAL TREE NETWORK WORD SPOTTING SYSTEM
* Autho: KOSONOCKY, STEPHEN VICTOR
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; ARTIFICIAL INTELLIGENCE
* Abstr: A new classifier is described that combines the
 discriminatory ability of the neural tree network (NTN)
 with the Gaussian mixture model to create a continuous
 density neural tree network (CDNTN). The CDNTN provides
 a method of blending a nonparametric tree type
 classifier with a parametric mixture model, to allow
 modeling complex distributions. The CDNTN is used within
 a Hidden Markov Model (HMM), along with a nonparametric
 state duration model to construct a continuous word
 spotting system for real time applications. The new word
 spotting system does not use a general background model,
 allowing construction of word models whose performance
 is independent of the number of models in the
 recognition system, supporting a direct parallel
 implementation. Although HMM word spotting systems are
 shown to provide good performance when sufficient
 training data is available, for applications where
 background speech data is not available or only a
 limited numbers of training tokens are available, the
 CDNTN word spotting system is shown to out perform
 comparable HMM systems. The CDNTN is also shown to
 provide superior performance for a speaker verification
 task compared with systems using vector quantization.
#####################
* Score: 1521.207297656219
* Doc #: 3783
* Title: AUTOMATED ANALYSIS OF NUCLEAR MEDICINE IMAGES: TOWARDS ARTIFICIAL INTELLIGENCE SYSTEMS
* Autho: SLOMKA, PIOTR JAN
* Desc : HEALTH SCIENCES, RADIOLOGY; BIOPHYSICS, MEDICAL
* Abstr: Automated methods for the analysis of nuclear medicine
 images could provide an objective diagnosis, and means
 to transfer sophisticated expertise to less experienced
 centres. The goal of this study was to develop software
 methods for the automated analysis of (a) Quality
 Control (QC) images, and (b) myocardial perfusion
 tomography images.
 The system for the automated analysis of QC images was
 based on feature extraction algorithms, which provided
 input to a higher level diagnostic expert system.
 Several features characterizing QC images were defined.
 Rule-based and object-oriented expert systems were
 created to guide personnel in QC procedures, detect
 gamma camera faults, and suggest corrective actions. An
 object-oriented representation of knowledge allowed a
 natural representation and classification of image
 features, artefacts, and other concepts used in this
 knowledge domain. The feature extraction algorithms
 combined with a prototype expert system could perform
 diagnosis of gamma camera faults and QC procedure errors
 on a limited set of examples.
 Computer-aided analysis of myocardial perfusion images
 was accomplished by creating three-dimensional (3-D)
 reference templates, to which patient's images could be
 automatically aligned using image registration
 algorithms. The templates included a normal distribution
 of activity and perfusion maps corresponding to specific
 coronary arteries. The quantification was done by a 3-D
 region-growing procedure that outlined perfusion defects
 in test-patients based on differences from the normal
 templates. Alignment and quantification methods of
 myocardial perfusion images were successfully tested on
 a group of 168 angiographically correlated patients.
 Perfusion defects were characterized in terms of numeric
 parameters, thus avoiding subjective visual assessment.
 The location of defects relative to the expected
 hypoperfusion sites was also established.
 Analytical and artificial intelligence software methods
 can be used for automated interpretation of QC and
 cardiac images. Object-oriented methods are suitable for
 encoding the knowledge required for computer-aided
 analysis of QC images. A comprehensive and fully
 automated analysis of cardiac perfusion images is
 possible by comparison of patient data to 3-D reference
 models.
#####################
* Score: 1521.207297656219
* Doc #: 3544
* Title: SYNTHESE DE RESEAUX DE NEURONES ARTIFICIELS POUR RESOUDRE DES PROBLEMES D'OPTIMISATION NP-COMPLETS: APPLICATIONS EN VLSI
* Autho: AOURID, SIDI MOHAMED
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; ARTIFICIAL INTELLIGENCE
* Abstr: This thesis proposes a mathematical method to synthesise
 and analyse a neural architecture for pseudo-boolean
 optimization. Results proposed by Hopfield have shown
 many drawbacks concerning the stability and the quality
 of the solutions. The Hopfield's model can solve only
 small problems. This is due to the energy function
 defined by Hopfield for his system. This energy has many
 local minima which are in most cases irrelevant to the
 solution of the original problem. To overcome these
 difficulties, a new energy function is developed based
 on the equivalence between concave and integer
 programming problems. We have also used penalty methods
 to transform a constrained problem into an unconstrained
 one. The energy function that we have obtained consists
 of three terms: the first term is the cost function for
 the original problem, the second term is obtained
 according to the equivalence between concave and integer
 programming and the last term is the violated
 constraints. Next, we have shown that our energy
 function is continuously differentiable and also a
 Lyapunov function which represent an important impact in
 neural networks synthesis. This energy is also
 associated with two parameters that must be chosen
 adequately to guarantee a good solution. We have shown
 that the choice of these parameters depends on the
 method used to minimize the energy function. In fact, by
 using neural networks to minimize its energy, we have
 shown that the concavity of the energy function can be
 omitted. This allows us to propose an efficient method
 to fix the parameters very easily.
 In order to ensure the convergence and the stability of
 the system, we have drawn two conditions that our system
 must satisfy at any moment. These two conditions
 rewritten otherwise are equivalent to the Khun-Tucker
 optimality conditions. To evaluate the performances of
 our network, many linear and quadratique 0-1 problems
 under inequalities constraints are tested and good
 results are obtained.
 Finally, we have solved a static compaction problem for
 combinational circuits. This problem consists of, given
 a test vectors set with some fault coverage, determining
 a subset of test vectors without compromising the fault
 coverage. Different methods for this problem are known
 to be inefficient and very expensive regarding the
 dynamic compaction where the compaction is performed
 during test generation phase. To solve this problem, we
 have shown first, an equivalence between static
 compaction and set covering problem. Then, the
 equivalent set covering problem is solved by a neural
 network that we have developed. The obtained results
 show the advantages of this equivalence for static
 compaction and they also show that our network can deal
 with large scale problems. (Abstract shortened by UMI.)
#####################
* Score: 1521.207297656219
* Doc #: 3408
* Title: LATERAL VEHICLE CO-PILOT TO AVOID UNINTENDED ROADWAY DEPARTURE
* Autho: PILUTTI, THOMAS E.
* Desc : ENGINEERING, MECHANICAL; ENGINEERING, SYSTEM SCIENCE; ENGINEERING, AUTOMOTIVE; ARTIFICIAL INTELLIGENCE
* Abstr: A detailed investigation of road departure warning
 systems is presented. A method known as time-to-lane-
 crossing is compared with rumble strips placed a fixed
 distance from the road edge, and is found to provide
 enhanced performance in terms of reduced false warnings
 and increased warning anticipation. A new approach,
 called variable rumble strip (VRBS), is proposed as an
 electronic implementation of rumble strips where the
 rumble strip threshold is allowed to vary according to
 the risk of the vehicle departing the road. The rule-
 based system is realized using a fuzzy logic structure.
 Performance of the VRBS system is similar to that of the
 time-to-lane-cross based approach, but requires less
 sensor information. Performance is measured by
 comparison with a validation warning set generated by
 static rumble strip warnings, and subjective assessment
 of road departure criticality. The algorithms are tested
 on 12 two-hour driving runs conducted in a full-vehicle
 driving simulator.
 One extension of the VRBS system involves an estimate of
 driver lane-keeping performance used to modify the VRBS
 threshold adjustment. The estimate, based on the
 standard deviation of lateral vehicle position, is
 effective at increasing the anticipatory warning time,
 and is readily implemented in the fuzzy rule structure.
 A separate driver modeling effort was undertaken using a
 system identification approach to develop a driver
 model, and to update its parameters during driving.
 Although preliminary driving simulator results indicated
 that changes in the damping ratio, natural frequency,
 and DC gain of such a model may be useful indicators of
 driver fatigue, the identified model parameters were
 found to not exhibit the expected trends as lane-keeping
 performance deteriorated on more extensive data sets.
 Addition of an intervention function is the topic of a
 second extension, and examines the usefulness of a brake
 steer system which uses differential brake forces for
 steering intervention. The steering function achieved
 can be used to provide limited control authority on
 vehicle lateral position. Control design models for the
 vehicle and the brake system are presented. Computer
 simulation results, using a nonlinear seven degree-of-
 freedom vehicle model are included, and show the
 feasibility and limitations of brake steer.
#####################
* Score: 1521.207297656219
* Doc #: 3315
* Title: AN ARCHITECTURE FOR COLLABORATIVE PROBLEM-SOLVING CONTROL IN ASSOCIATE SYSTEMS
* Autho: FU, MICHAEL CHIN-MING
* Desc : COMPUTER SCIENCE; ENGINEERING, INDUSTRIAL; ARTIFICIAL INTELLIGENCE
* Abstr: In the past, AI systems have strived to automate problem-
 solving processes completely. However, in recent years
 researchers have come to realize that it is not always
 possible or desirable to aim for total automation.
 Researchers are realizing the importance of human-
 computer collaborative systems in which the human and
 the computer work as a team in solving problems. This
 approach raises the question of how to design systems
 that support effective collaborative problem-solving
 between humans and computers. The primary contribution
 of this thesis is an architecture for coordination of
 collaborative problem-solving (CO-SOLVE) for an
 important class of human-computer collaborative systems
 called associate systems. Associate systems are
 knowledge-based systems which share the cognitive
 workload with their human partners. Designers of
 associate systems must deal with the complexities of
 integrating mixed-initiative (i.e. human and computer)
 control with the general issues of problem-solving
 control faced by traditional AI systems. CO-SOLVE
 provides mechanisms for attention synchronization and
 collaborative alternatives exploration. The Attention
 Synchronization Model (ASM) allows the system to track
 (rather than direct) the user's activities in order to
 provide advice relevant to the current user activities.
 The Collaborative Alternatives Exploration Model (CAEM)
 is a mixed-initiative approach to exploring large,
 complex solution spaces. In this collaborative framework
 the user serves as solution evaluator and system
 controller and the computer as solution alternative
 generator. System developers using CAEM explicitly lay
 out steps in the problem-solving process for a given
 task and define points of human-computer interaction
 within the sequence of process steps. The other
 contribution of this thesis is a proof-of-concept
 prototype of CO-SOLVE, called SEDAR, which is
 implemented for a real-world, complex domain (flat and
 low-slope roof design). Two evaluations were conducted
 on SEDAR. The first assessed the effectiveness and
 usability of the ASM and its critiquing strategies as
 implemented in SEDAR. The evaluation showed that SEDAR
 reduced the error rate of experienced architects and
 also which advising strategies they preferred. The
 second evaluation assessed the effectiveness of the CAEM
 as implemented in SEDAR and showed that SEDAR (1) helped
 experienced architects reduce the amount of time spent
 developing solutions, and (2) increased the number of
 alternatives searched in the solution space.
#####################
* Score: 1521.207297656219
* Doc #: 3154
* Title: AN ARTIFICIAL INTELLIGENCE APPLICATION OF BACKPROPAGATION NEURAL NETWORKS TO SIMULATE ACCOUNTANTS' ASSESSMENTS OF INTERNAL CONTROL SYSTEMS USING COSO GUIDELINES
* Autho: O'CALLAGHAN, SUSANNE
* Desc : BUSINESS ADMINISTRATION, ACCOUNTING; ARTIFICIAL INTELLIGENCE
* Abstr: The objective of this study was to explore a form of
 artificial intelligence, neural network modelling, to
 examine variables that are crucial to technological
 implementation in accounting settings. The experiment
 utilized an accounting framework, assessing internal
 controls under COSO guidelines.
 The results of this experiment suggest that a neural
 network model can be developed such that the decision
 processes of external auditors in assessing internal
 controls can be reasonably modelled. A significant
 difference exists between the classification precision
 of network models (a) using one hidden layer as opposed
 to two hidden layers, (b) between models with differing
 configurations of neurons within the hidden layer(s) and
 (c) a regression model for certain conditions.
 A study of the incorrect decisions made by a neural
 network indicates that while reliance on a network would
 result in making some incorrect decisions, the threat of
 over-relying on internal controls is not extremely high.
 By eliminating noise in the research instrument, a
 network can be modelled that will be able to predict a
 higher number of correct assessments than was possible
 with the full experimental model.
 A sensitivity analysis revealed that none of the five
 COSO inputs individually has an extreme effect on the
 neural network's ability to make internal control
 assessments. Self assessments by auditors who rate
 themselves as experts reveals that their models have a
 higher prediction rate at assessing internal controls
 than does the network developed from responses of lower
 self-assessed experts. Analysis reveals that auditors
 are extremely conservative in their response, especially
 in assessing controls over compliance with rules and
 regulations. Neural networks that were developed using
 effectiveness of internal controls as an outcome
 measure, had higher accuracy predictions than networks
 developed using quality of internal controls as an
 outcome measure.
 This research demonstrates the usefulness of applying a
 neural network paradigm in assessing the effectiveness
 of internal control systems.
#####################
* Score: 1521.207297656219
* Doc #: 3132
* Title: CHARACTERIZATION AND REPRESENTATION OF FUNCTIONAL REQUIREMENTS AND FUNCTIONAL TOLERANCING FOR CONCURRENT DESIGN AND MANUFACTURING
* Autho: CHEN, BAOSHENG
* Desc : ENGINEERING, MECHANICAL; ENGINEERING, INDUSTRIAL; ARTIFICIAL INTELLIGENCE
* Abstr: The proposed dissertation research is designed to
 investigate two issues that are directly related to the
 concurrent design and manufacturing of mechanical
 products and systems. The focus is on understanding the
 functional structure of modern mechanical products and
 on modeling their required functions so as to support
 the various activities in their design and
 manufacturing.
 The characterization of the functional requirements of
 both mechanical components and systems has been studied
 in this dissertation research. The hierarchy analysis,
 the decomposition, and the dependence check of the
 functional requirements are used as three major issues
 in the characterization process. To represent a group of
 functional requirements, a functional structure and a
 descriptive model which includes functional intent,
 characteristics, and target value or ideal state are
 proposed. The representation scheme can be implemented
 along with the design model of a product to provide
 sufficient information for downstream product
 activities.
 To support the subsequent production activities, based
 on the understanding of the various functional
 requirements, an approach of tolerancing for function is
 presented. The tolerancing for function approach
 attempts to determine the possible tolerance
 specifications for a product directly according to its
 functional requirements and their allowable variations.
 Based on the proposed tolerancing for function approach,
 a framework for functional tolerancing is proposed which
 can be implemented in a computer-integrated design and
 manufacturing environment. The framework includes four
 inter-related modules: identification, transformation,
 tolerancing, and specification modules.
 The proposed functional framework provides a basis for
 the characterization and representation of the
 functional requirements in mechanical design. It will
 also provide a basis for integrating design intent into
 the product cycle so that higher level of decision
 making and complex reasoning for the determination of
 optimum product design and process planning become
 possible. The proposed tolerancing for function approach
 can be used to support the subsequent production
 activities, and establish a foundation for continuing
 research on automatic tolerancing. By using the proposed
 functional tolerancing approach, the current practice of
 tolerance representation and specification can be
 extended and applied to the products that are
 sophisticated in function and/or complicated in
 geometry.
#####################
* Score: 1521.207297656219
* Doc #: 2888
* Title: AN INTELLIGENT CONTROL SYSTEM FOR RESISTANCE SPOT WELDING USING FUZZY LOGIC AND NEURAL NETWORK
* Autho: JOU, MIN
* Desc : ENGINEERING, MECHANICAL; ENGINEERING, MATERIALS SCIENCE; ARTIFICIAL INTELLIGENCE; ENGINEERING, INDUSTRIAL
* Abstr: Resistance spot welding is subject to numerous
 predictable and unpredictable variables within and
 between welding cycles. These can arise from: material
 types, composition and thickness tolerances, and surface
 finishes and cleanliness, joint geometry, fit-up and
 weld spacing and current-shunting; electrode deformation
 and wear, and fluctuations in machine cooling water flow
 rate and temperature or welding and forging pressure;
 and fluctuations in primary (line) voltage and current.
 Any or all of these complicate automation, reduce weld
 quality, demand overwelding (i.e., production of more
 welds than are structurally needed, if each was
 perfect), and drive up production costs. For this
 reason, ensuring weld quality through on-line, real-time
 process control has been and remains a major challenge
 and goal.
 The objective of this research is to develop an
 intelligent control system to ensure weld quality for
 the resistance spot welding process. The chosen system
 consists of a neural network to generate electrode
 displacement and velocity as a function of welding time
 and a fuzzy logic controller to execute control commands
 based on deviations from ideal behavior caused by
 process variations or errors. This approach was taken to
 preclude the need for a precise model of the complex
 physics involved in the process as well as to circumvent
 the need for experimentally generated curves of
 electrode displacement for each and every material joint
 combination.
 In this project, electrode displacement was used as the
 signal to indicate weld quality as affected by the
 growth of the weld nugget in real time. A series of
 experiments were conducted to explore how changes of a
 key controllable parameter (i.e., % heat input or power)
 affect a measurable signal (i.e., electrode
 displacement) for various sheet steels used in the
 automotive industry. An artificial neural network has
 been developed to generate the relationship between %
 heat input and electrode displacement. A fuzzy logic
 control system based on this neural network has also
 been developed. The output from the fuzzy logic
 controller (FLC) is the control action which is then
 used to control the power delivered into the weld.
 Computer simulation results show that a fuzzy logic
 control system can tolerate and compensate for in-
 process uncertainties and make an acceptable joint. With
 this FLC, the common practice in RSW to overdesign the
 total number of welds needed to obtain desired strength
 of joints can be eliminated, with attendant cost
 savings. Automation of the process in a typical
 production environment is also advanced by precluding
 the need for a precise model of the physics of the
 process, by employing fuzzy logic rules instead. The
 need for costly and time-consuming experiments to
 generate a curve of electrode displacement versus
 welding time is circumvented by using a trained neural
 network. Together, a neural network and fuzzy logic
 represent a new approach for achieving control of the
 complex process of resistance spot welding in automated
 assembly environments typified by the automobile
 industry that could represent an important new control
 strategy for such complex processes.
#####################
* Score: 1521.207297656219
* Doc #: 2797
* Title: AN INTELLIGENT TOOL FOR EXPERIENCED PROGRAMMERS LEARNING ADA
* Autho: FIX, VIKKI LORRAINE
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: When experienced programmers learn a new language, their previous
 planning knowledge results in both positive and negative transfer to the
 new language. A tool to help them learn the new language must include
 planning as an important component and the planning help must consider the
 previous planning knowledge of the programmer. Several existing programming
 tutors guide novice users through a few simple problems and spend at least
 as much time on coding as on planning. This dissertation describes an
 intelligent tool built to help experienced Pascal or C programmers learn to
 use Ada packages to create reusable software components.
      Empirical studies were completed to determine the knowledge base of
 correct and buggy plans used by programmers as they create modules which
 use packages. This knowledge base is used to model the student and to
 diagnose errors. Given an exercise, the student creates a solution by
 working from high-level plans to more detailed plans to code. This method
 of solution simulates a programmer using pseudocode for planning and
 replacing plans with code when the way to implement a plan becomes clear.
 When it is clear that the student is using a plan from a previous language
 that is impossible or extremely suboptimal in Ada, the tool intervenes to
 get the user back on a productive path. The tool uses model-tracing for
 diagnosis. In some situations, students choose from a menu to indicate
 their mental state, and in other situations, plan recognition is used to
 determine their mental state from their intermediate actions.
      The initial version of the tool was evaluated in a study in which
 programmers at the senior or graduate level used the tool to solve one
 problem and then completed transfer tasks. The results of the study showed
 that subjects learned package concepts using the tool. The results also
 suggest improvements for the next version of the tool.
#####################
* Score: 1521.207297656219
* Doc #: 2701
* Title: EXPLOITING LOW-LEVEL LINGUISTIC KNOWLEDGE IN NEURAL NETWORK SPEECH RECOGNITION
* Autho: DICKEY, MARTIN
* Desc : COMPUTER SCIENCE; LANGUAGE, LINGUISTICS; ARTIFICIALINTELLIGENCE
* Abstr: This project explores ways in which low-level linguistic knowledge
 sources may be exploited in speaker-independent continuous speech
 recognition systems, based on a neural network classifier with no hidden
 Markov Model (HMM) hybridization. Starting point is a multi-layer
 perceptron (MLP) with one hidden layer. All data is drawn from the TIMIT
 corpus of American speech. Signal processing is conventional. A new network
 metric based on rank is introduced. Novel training techniques include lazy
 error propagation, a maximum tolerance criterion, and rank-based dynamic
 adjustment of eta. These concepts are first validated using the 8-3-8
 encoding problem. Speech-specific innovations include sympathetic phonetic
 context training and special protocols for the construction of large
 training sets. The phonetic context technique is shown to be helpful; two
 other training proposals, bypass and perturbation, are not. Networks of
 this type, trained for the entire set of 60 TIMIT phonetic symbols, achieve
 up to 30% frame-level accuracy on unfamiliar sentences. Given knowledge of
 segment boundaries, this raw identification rate can be improved by about
 50% with the application of segmental duration knowledge and by 100% or
 more when first-order frequency is also used. Knowledge source were derived
 directly from TIMIT and the duration distributions approximated. Next, the
 question of labeling without prior boundary knowledge is addressed. An
 otherwise exponential search is first reduced to O(N$\sp3$) by means of a
 dynamic programming algorithm, and then to O(N), with some loss of solution
 quality and an O(N$\sp2$) set-up. Search heuristics require a rough initial
 segmentation, which is produced by a novel method based on ragged edge
 properties of adjacent segments. Further knowledge sources explored include
 frequency of occurrence of pairs of phonetic symbols. A recurring theme is
 how various parameters and knowledge sources are most effectively scaled
 before application. The transcriptions of complete sentences produced by
 these techniques contain many errors but are frequently intelligible, even
 though no explicit phonological, lexical or syntactic constraints have been
 applied.
#####################
* Score: 1521.207297656219
* Doc #: 2002
* Title: ... ON THE APPLICATION OF ARTIFICIAL INTELLIGENCE TECHNIQUES TO HEAT EXCHANGER DESIGN
* Autho: WHITE, PETER
* Desc : COMPUTER SCIENCE; ENGINEERING, GENERAL
* Abstr: Traditional computer-based preliminary design tools written in
 FORTRAN-like codes are expensive to develop and maintain because the design
 knowledge is embedded in the application code. Engineering designers have
 become interested in Artificial Intelligence (AI), particularly the
 knowledge-based systems, because the pattern of AI programs is to separate
 knowledge and knowledge processing procedures.
      In this work, ideas pioneered in the PAPER AIRPLANE design tool system
 have been adopted, in which design equations are represented in a
 data-orientated manner and a domain-independent control strategy is used
 for performing design calculations. This novel approach allows the designer
 to consider a wide range of design alternatives, by selecting equations
 relevant to the specific design object in mind from a library of equations
 pertinent to a broad design discipline.
      The approach has been validated by investigating a number of
 double-pipe heat exchanger design and analysis calculations. A powerful
 mechanism for representing constraints on design variables has been
 developed, ensuring the integrity of the calculations. This has permitted
 the incorporation of a constrained multivariate optimisation procedure into
 the design tool. Results of designs optimised for a range of objective
 functions are presented.
#####################
* Score: 1521.207297656219
* Doc #: 1941
* Title: THE DESIGN, DEVELOPMENT, AND IMPLEMENTATION OF A KNOWLEDGE-BASED SYSTEM FOR CONTROLLING HEALTH BENEFIT COSTS
* Autho: MARTIN, JACK L.
* Desc : BUSINESS ADMINISTRATION, GENERAL; ARTIFICIAL INTELLIGENCE
* Abstr: Firms with cost-plus medical insurance administered by outside parties
 often fail to review the administrator's performance effectively. In many
 cases the firms do not know the extent of overpayments since they lack the
 expert knowledge required to evaluate claims paid.
      This research examines the use of a knowledge base for identifying
 claim payment errors. The system's knowledge base is drawn from the areas
 of claims processing, auditing, and diagnosis and procedure coding
 practices. Once potential errors are identified, a mathematical program is
 used to select claims to maximize expected benefits subject to various
 firm-specific claim review limitations. A Fortune 100 firm's claims
 database is used to test the system.
#####################
* Score: 1521.207297656219
* Doc #: 1807
* Title: SAMPLING ISSUES FOR CLASSIFICATION USING NEURAL NETWORKS
* Autho: SUBRAMANIAN, VENKAT
* Desc : BUSINESS ADMINISTRATION, MANAGEMENT; INFORMATION SCIENCE;COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Neural networks are information processing systems patterned after the
 highly interconnected neural systems of the human brain. They are trained
 through examples rather than being programmed. The currently popular
 training algorithm, called back propagation, suffers from serious drawbacks
 such as lack of robustness, slow convergence, especially for large
 networks, and lack of reliability. Since neural network training is an
 optimization problem, this dissertation establishes the suitability of
 proven nonlinear optimization methods and the superiority of these methods
 over back propagation.
      Neural networks, by their nature of being able to generalize and
 resist noisy data, are particularly appropriate for pattern recognition
 problems. One prominent pattern recognition problem is the classification
 problem which assigns an observation, based on a set of attributes, to one
 of finite groups. For example, the classification problem is used in
 accepting or rejecting credit application based on an applicant's personal
 and financial data. This dissertation compares the performance, in terms of
 correct classifications, of neural networks against that of the traditional
 multidimensional discriminant analysis methods. The results show that even
 under the perfect assumptions for the traditional methods, neural networks
 compared favorably.
      The question of how to design neural networks for classification is
 the next main topic of this dissertation. The issues investigated include
 sampling strategy and network architecture. Sampling strategy refers to the
 decisions on sample size, sample composition, and variance-covariance
 matrices of attributes. Network architecture refers to the number of nodes
 and the interconnections among the nodes in a network. A rigorous and
 extensive experiment was conducted to answer questions on these issues.
 Design principles based on these empirical results were established.
#####################
* Score: 942.7329913522993
* Doc #: 4273
* Title: REDUCING MATCH TIME VARIANCE IN PRODUCTION SYSTEMS WITH HAL
* Autho: LEE, POU-YUNG
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Existing match algorithms approach the matching process
 similarly to the querying process of relational
 databases but incrementally. The potentially
 combinatorial nature of the matching process and the
 difference in the quantity of data each rule needs to
 process introduce match time variance in the matching
 cycles. Current match algorithms utilize local matching
 support networks containing redundant working memory
 elements. This compounds the variance by increasing
 further the quantity of data that need to be matched.
 Large match time variance makes them unsuitable for real-
 time applications requiring timing constraint. We
 introduce the Heuristically-Annotated-Linkage (HAL)
 match algorithm to reduce match time variance. HAL
 treats rules and classes as objects, or nodes, in only
 one global bipartite-graph-like connection and
 communication scheme to reduce data redundancy. In
 addition, HAL is suitable for real-time applications
 because it is capable of immediate characterization of
 any datum upon arrival to allow immediate execution of
 timing constrained actions.
#####################
* Score: 942.7329913522993
* Doc #: 4214
* Title: ON INTEGRATING APPRENTICE LEARNING AND REINFORCEMENT LEARNING
* Autho: CLOUSE, JEFFERY ALLEN
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Apprentice learning and reinforcement learning are
 methods that have each been developed in order to endow
 computerized agents with the capacity to learn to
 perform multiple-step tasks, such as problem-solving
 tasks and control tasks. To achieve this end, each
 method takes differing approaches, with disparate
 assumptions, objectives, and algorithms. In apprentice
 learning, the autonomous agent tries to mimic a training
 agent's problem-solving behavior, learning based on
 examples of the trainer's action choices. In an attempt
 to learn to perform its task optimally, the learner in
 reinforcement learning changes its behavior based on
 scalar feedback about the consequences of its own
 actions.
 We demonstrate that a careful integration of the two
 learning methods can produce a more powerful method than
 either one alone. An argument based on the
 characteristics of the individuals maintains that a
 hybrid will be an improvement because of the
 complimentary strengths of its constituents. Although
 existing hybrids of apprentice learning and
 reinforcement learning perform better than their
 individual components, those hybrids have left many
 questions unanswered. We consider the following
 questions in this dissertation. How do the learner and
 trainer interact during training? How does the learner
 assimilate the trainer's expertise? How does the
 proficiency of the trainer affect the learner's ability
 to perform the task? And, when during training should
 the learner acquire information from the trainer? In our
 quest for answers, we develop the A scSK FOR H scELP
 integrated approach, and use it in our empirical study.
 With the new integrated approach, the learning agent is
 significantly faster at learning to perform optimally
 than learners employing either apprentice learning alone
 or reinforcement learning alone. The study indicates
 further that the learner can learn to perform optimally
 even when its trainer cannot; thus, the learner can
 outperform its trainer. Two strategies for determining
 when to acquire the trainer's aid show that simple
 approaches work well. The results of the study
 demonstrate that the A scSK FOR H scELP approach is
 effective for integrating apprentice learning and
 reinforcement learning, and support the conclusion that
 an integrated approach can be better than its individual
 components.
#####################
* Score: 942.7329913522993
* Doc #: 4204
* Title: NEURAL NETWORK SIGNAL PROCESSING TECHNIQUES FOR ADVENTITIOUS LUNG SOUND CLASSIFICATION
* Autho: FORKHEIM, KEVIN EDWARD
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE; ENGINEERING, BIOMEDICAL
* Abstr: The purpose of this research is to compare the
 performance of different representations of lung sounds
 and different types of neural network models at
 classifying wheezes. The result of combining multiple
 neural networks is also investigated.
 Lung sounds were recorded from the thorax region and
 divided into segments of 0.2 seconds. These segments
 were then classified as either containing a wheeze or
 not containing a wheeze by a trained clinician. A neural
 network was trained and tested using the raw signal,
 filtered, and Fourier Transform representations so that
 the best representation for the neural network could be
 found. Once the best representation was found, five
 different types of neural networks--Backpropagation
 (BP), Self Organizing Maps (SOM), Learning Vector
 Quantization (LVQ), Probabilistic Networks (PNN), and
 Radial Basis Functions (RBF)--were compared to find the
 best neural network for classifying wheezes. Experiments
 were also performed with different post-processing
 techniques such as thresholding and combining multiple
 classifiers. (Abstract shortened by UMI.)
#####################
* Score: 942.7329913522993
* Doc #: 212
* Title: DEVELOPMENT OF AN INTELLIGENT ANALYSIS TOOL FOR CONTINUOUS SIMULATION SYSTEMS
* Autho: GUDIPATI, AMBAPRASAD
* Desc : INFORMATION SCIENCE; ENGINEERING, AGRICULTURAL; ARTIFICIAL INTELLIGENCE; COMPUTER SCIENCE
* Abstr: Generally expert simulation systems contain heuristic
 rules in their knowledge base. These rules are just
 summaries of experiences and may not provide context
 sensitive information to the expert system component.
 When these rules are used, the expert system component
 may not analyze the results of simulation in a proper
 way and it may not provide appropriate input parameters
 to the simulation, as a result of which the simulation
 may not be used effectively. The hypothesis of this
 research is that an expert advisor with the assistance
 of an intelligent analysis tool can utilize a simulation
 to make decisions in a manner similar to that used by
 human experts. To validate this hypothesis, a PIX
 Advisor was developed as part of the GOSSYM-COMAX
 simulation-based expert system. The analysis tool of the
 PIX Advisor utilizes a knowledge base and an analytical
 model in making recommendations. With the successful
 development of the PIX Advisor, the author demonstrated
 the utility of combining heuristic rules and an
 analytical tool to direct the use of a complex
 simulation system with many interacting factors and
 feedback loops.
#####################
* Score: 942.7329913522993
* Doc #: 1791
* Title: A VISUAL SENSORY EVALUATION OF TWO PATTERN GRADING METHODS
* Autho: BYE, ELIZABETH KERSCH
* Desc : HOME ECONOMICS; ARTIFICIAL INTELLIGENCE
* Abstr: Current apparel production methods grounded in a tradition of
 master-apprentice training and intuition are being challenged by the
 introduction of Artificial Intelligence. Much of the expert knowledge that
 is needed to develop such computer systems is related to the visual
 decisions that are made to create a garment. To gain access to this
 information, new methods are needed which predict consumer response and
 make connections to changes in technology that go beyond current practices.
      A sensory evaluation methodology was used to determine if visual
 perceptual differences existed from the results of two methods of pattern
 grading, traditional and proportional. Pattern grading is the process of
 increasing or decreasing the size in which a garment was originally
 designed, according to a set of body measurements to develop a range of
 sizes for mass production while maintaining the visual effect of the
 original sample size.
      Thirteen candidates took part in a training session and screening test
 to determine a level of consistency in evaluating differences. An expert
 panel of eleven members was used to perform evaluations of images
 representing traditional and proportional grading of Misses sizes 8 to 20.
 Three images were used to represent horizontal proportion, vertical
 proportion, and proportion of details.
      Seventy-eight paired computer generated photographic images were
 evaluated for visual differences on a six point scale. Data were analyzed
 using a t-Test to determine if there were visual perceptual differences
 between the sample and the graded sizes, and between the individually
 graded sizes for each of the three images.
      Results indicated that neither traditional more proportional grading
 maintained the visual effect referenced in the sample size across the
 entire size range. Conclusions suggest that there be some redefinition of
 the goal of pattern grading which takes into account the desired visual
 effect for a variety of size and body variations.
      Sensory evaluation provided a systematic framework for the assessment
 of an often intuitively perceived physical characteristic: appearance. The
 development of sensory test designs specified for apparel may help expand
 the use of this methodology. The more that is known about how a consumer
 perceives apparel, the better the industry will be able to meet the needs
 of the consumer through the development of new technology.
#####################
* Score: 942.7329913522993
* Doc #: 1678
* Title: ARTIFICIAL INTELLIGENCE AND OPTIMIZATION SOLUTIONS TO MULTI-CRITERIA OPERATOR BINDING
* Autho: FOGG, DENNIS CHEE-YUEN
* Desc : COMPUTER SCIENCE; ENGINEERING, ELECTRONICS AND ELECTRICAL;ARTIFICIAL INTELLIGENCE
* Abstr: Operator binding is one task in the high-level synthesis of computer
 architectures. It consists of selecting an implementation for each operator
 (or module). In this dissertation operator binding is evaluated with
 respect to circuit delay, circuit area, and (in some cases) number of
 implementation types used. Two solutions are presented. The first is an
 artificial intelligence (AI) framework, called Heuristically Guided
 Enumeration (sc HGE), that is applicable to any parametric design problem
 with multiple criteria. The fundamental obstacle in multi-criteria design
 is preferences among criteria cannot be specified precisely. Instead of
 requiring precise specifications, scHGE generates actual designs and lets
 the user choose the most preferred one. Expert knowledge is used to
 generate designs near the optimal frontier. Enumerating around those
 designs compensates for inadequacies in the expert knowledge and presents
 realizable tradeoffs among performance criteria. The user directs the
 generation of designs by defining regions of interest and by limiting the
 cardinality of the designs created. scHGE encourages the exploration of the
 design space by generating fast approximations to the frontier. An
 implemented system enumerates one millionth of the design space in 23
 seconds yet produces near-optimal results.
      The second solution formulates the operator binding task as a
 mathematical programming problem. The key insight is to approximate the
 discrete implementation choices with a convex hull, so the discrete
 optimization problem becomes a linear programming (LP) problem. The LP
 solution defines a lower bound on the optimal design, and subsequent
 processing to discretize the solution produces a tight upper bound. The
 convex hull approximation is combined with Leiserson and Saxe's retiming
 algorithm to simultaneously bind operators and choose pipeline register
 placements to minimize circuit area. The new algorithm, called
 retimed-binding, uses mixed-integer linear programming and can explore the
 optimal tradeoffs among circuit area, clock period, and latency.
      In addition, ideas for a new search method called sample search are
 presented. Traditional search methods are inadequate for design, because
 partial designs cannot be evaluated accurately. Sample search evaluates
 partial designs by sampling the design space that the partial design
 entails. Heuristics guide the samples to the best solutions first. The
 desirable partial designs are investigated with more scrutiny using denser
 sampling. Sample search has yet to be implemented.
      A significantly revised version of this dissertation will be available
 as MIT Laboratory for Computer Science technical report MIT/LCS/TR-480.
 (Copies available exclusively from MIT Libraries, Rm. 14-0551, Cambridge,
 MA 02139-4307. Ph. 617-253-5668; Fax 617-253-1690.)
#####################
* Score: 942.7329913522993
* Doc #: 1651
* Title: NEURAL NETWORKS FOR PERCEPTUAL GROUPING
* Autho: SARKARIA, SARBJIT SINGH
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: A number of researchers have investigated the application of neural
 networks to visual recognition, with much of the emphasis placed on
 exploiting the network's ability to generalise. However, despite the
 benefits of such an approach it is not at all obvious how networks can be
 developed which are capable of recognising objects subject to changes in
 rotation, translation and viewpoint. In this study, we suggest that a
 possible solution to this problem can be found by studying aspects of
 visual psychology and in particular, perceptual organisation. For example,
 it appears that grouping together lines based upon perceptually significant
 features can facilitate viewpoint independent recognition.
      The work presented here identifies simple grouping measures based on
 parallelism and connectivity and shows how it is possible to train
 multi-layer perceptrons (MLPs) to detect and determine the perceptual
 significance of any group presented. In this way, it is shown how MLPs
 which are trained via backpropagation to perform individual grouping tasks,
 can be brought together into a novel, large scale network capable of
 determining the perceptual significance of the whole input pattern. Finally
 the applicability of such significance values for recognition is
 investigated and results indicate that both the MLP and the Kohonen Feature
 Map can be trained to recognise simple shapes described in terms of
 perceptual significances.
      This study has also provided an opportunity to investigate aspects of
 the backpropagation algorithm, particularly the ability to generalise. In
 this study we report the results of various generalisation tests. In
 applying the backpropagation algorithm to certain problems, we found that
 there was a deficiency in performance with the standard learning algorithm.
 An improvement in performance could however, be obtained when suitable
 modifications were made to the algorithm. The modifications and consequent
 results are reported here.
#####################
* Score: 942.7329913522993
* Doc #: 1494
* Title: SIMULATION-BASED UNDERSTANDING OF TEXTS ABOUT EQUIPMENT
* Autho: KSIEZYK, TOMASZ BARTLOMIEJ
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE; EDUCATION,LANGUAGE AND LITERATURE
* Abstr: This thesis presents a natural language understanding system,
 operating in the domain of equipment consisting of mechanical, hydraulic,
 and electrical elements. The task of the system is to analyze reports
 regarding the failure, diagnosis and repair of equipment. We argue that a
 general knowledge of equipment is not sufficient for a full understanding
 of such reports. As an alternative, we propose a system which relies on a
 detailed simulation model to support language understanding. We describe
 the structure of the model and emphasize features specifically required for
 language understanding. We show how this model can be used in analyzing and
 determining the referents for complex noun phrases describing equipment
 parts. We outline the data structures used for concepts which are mentioned
 in the text but which have no permanent representation in the model, and
 explain how they are created during the text analysis. Similarly, we
 discuss the data structures for representing the facts conveyed by the
 text, and provide algorithms for translating text expressing facts into
 their representations. We point out the importance of identifying the
 implicit temporal and causal relations in the text and show how the
 simulation capabilities of the model support this task. We present a
 dynamic graphical interface which gives the user insight into the way the
 input has been understood by the system. Finally, we indicate how our
 system may be extended to facilitate dynamic (i.e. during the analysis of
 text) extensions to its data base, and to assist the user in entering new
 equipment models. Most aspects of the discussed system were implemented on
 a Symbolics Lisp machine.
#####################
* Score: 942.7329913522993
* Doc #: 1474
* Title: THE IMPACT OF PROMOTION ON CONSUMER DECISION STRATEGIES
* Autho: SCHNEIDER, LINDA GAY
* Desc : BUSINESS ADMINISTRATION, MARKETING; ARTIFICIALINTELLIGENCE
* Abstr: The objective of this research is to understand and model the impact
 or promotional activity on consumer choice strategies at the individual
 level. In particular, we are interested in understanding how brand choice,
 purchase quantity, and interpurchase time interact.
      In this research, we argue that the appropriate unit of analysis is
 the individual, and that the impact of promotions on consumer decision
 making may be understood more fully by discovering the heuristics
 individuals use in making their brand decisions. We develop individual
 level models of consumer purchasing behavior by a methodology introduced
 into marketing by Currim, Meyer, and Le (1985). The Concept Learning System
 (CLS) employs an information entropy measure to develop a tree-like
 structure which details the conditions under which a choice will be made.
 The power of CLS lies in its ability to represent parsimoniously highly
 configural and idiosyncratic decision strategies.
      The key substantive findings in this dissertation are (1) deal prone
 consumers have weaker brand preferences relative to non-deal prones, (2)
 there are different types of deal proneness, (3) revealed brand preference
 plays a key role in determining the intensity of response to a promotion,
 (4) the previous purchase decision (that is the brand purchased, purchase
 quantity, and interpurchase time) is relatively unimportant in explaining
 the current purchase decision, (5) price seems to be an important mediating
 factor in the purchase quantity decision. Three methodological findings
 emerged. First, CLS was found to provide useful managerial insights and
 diagnostics. Second, the inferred trees seem to be quite stable. Finally,
 the position of independent variables in the trees seems to be stable to
 the fifth level.
#####################
* Score: 942.7329913522993
* Doc #: 1365
* Title: THE ROLE OF EXPLICIT CONTEXTUAL KNOWLEDGE IN LEARNING CONCEPTS TO IMPROVE PERFORMANCE
* Autho: KELLER, RICHARD MICHAEL
* Desc : COMPUTER SCIENCE
* Abstr: This dissertation addresses some of the difficulties encountered when
 using artificial intelligence-based, inductive concept learning methods to
 improve an existing system's performance. The underlying problem is that
 inductive methods are insensitive to changes in the system being improved
 by learning. This insensitivity is due to the manner in which contextual
 knowledge is represented in an inductive system. Contextual knowledge
 consists of knowledge about the context in which concept learning takes
 place, including knowledge about the desired form and content of concept
 descriptions to be learned (target concept knowledge), and knowledge about
 the system to be improved by learning and the type of improvement desired
 (performance system knowledge). A considerable amount of contextual
 knowledge is "compiled" by an inductive system's designers into its data
 structures and procedures. Unfortunately, in this compiled form, it is
 difficult for the learning system to modify its contextual knowledge to
 accommodate changes in the learning context over time.
          This research investigates the advantages of making contextual
 knowledge explicit in a concept learning system by representing that
 knowledge directly, in terms of express declarative structures. The thesis
 of this research is that aside from facilitating adaptation to change,
 explicit contextual knowledge can support two additional capabilities not
 supported in most existing inductive systems. First, using explicit
 contextual knowledge, a system can learn approximate concept descriptions
 when necessary or desirable in order to improve performance. Second, with
 explicit contextual knowledge, a learning system can generate its own
 concept learning tasks.
          To investigate the thesis, this study introduces an alternative
 concept learning framework--the concept operationalization framework--that
 requires various types of contextual knowledge as explicit inputs. To test
 this new framework, an existing inductive concept learning system (the LEX
 system  Mitchell et al. 81 ) was rewritten as a concept operationalization
 system (the MetaLEX system). This document describes the design of MetaLEX
 and reports the results of several experiments performed to test the
 system. Results confirm the utility of explicit contextual knowledge, and
 suggest possible improvements in the representations and methods used by
 the system.
#####################
* Score: 942.7329913522993
* Doc #: 1186
* Title: FOURIER TRANSFORM SPECTROMETRIC DIRECT MIXTURE ANALYSIS TECHNIQUES FOR IDENTIFICATION OF COMPONENTS OF BULK ORGANIC WASTE
* Autho: PUSKAR, MARK ALLEN
* Desc : HEALTH SCIENCES, PUBLIC HEALTH; 0800
* Abstr: This dissertation describes the application of direct infrared
 spectral interpretation techniques to the problem of identifying major
 components of waste organic mixtures. The principle hypothesis of this
 dissertation was that: Fourier Transform Infrared Spectrometry (FT-IR)
 spectral interpretation techniques can be used as a direct organic
 hazardous waste mixture analysis method. Initially, an artificial
 intelligence program PAIRS (Program for the Analysis of IR Spectra),
 designed to assist chemists in the identification of functional groups in
 neat infrared spectra, was evaluated as a screening technique for unknown
 organics. Results of this initial study indicated that classification was
 possible for many compound classes, however, compound specific rules
 written for a predefined training set of common compounds would be a more
 advantageous approach. PAIRS, along with an automated peak selection and
 spectral similarity subtraction routine, was adapted into a program for
 automated waste mixture interpretation. This new technique PAWMI (Program
 for Automated Waste Mixture Interpretation) has the ability to identify
 specific compounds present in mixtures and not just functional groups. The
 method also has the ability to use information from infrared peaks that
 have shifted in a mixture spectrum, with spectra generated with either
 transmission and internal reflection techniques. The qualitative limit of
 detection of minor components in mixtures tested ranged from 0.4% to 15%,
 depending on the spectral similarity of other components in the mixtures.
 The sensitivity of the method was 77%. Finally, two automated infrared
 techniques were reviewed as screening methods on eleven hazardous waste
 samples that had been previously analyzed by GC/MS. The results of the
 first method, forward searching spectra through a library of known neat
 spectra and commercial mixtures, showed that this technique can be a strong
 tool when the unknowns are relatively pure compounds, and can provide the
 ability to identify nonchromatographable components of mixtures that could
 not be determined by GC/MS. The results of the second method, PAWMI, were
 compared to those generated by GC/MS. The PAWMI technique had a sensitivity
 of 74%, a specificity of 98.1%, with 28% false positives. The applicability
 of these results to improved personnel protection strategies at hazardous
 waste sites is discussed in terms of a comparison with existing methods.
#####################
* Score: 831.8464223336346
* Doc #: 984
* Title: THE DESIGN AND IMPLEMENTATION OF A SELF-ADAPTING ALGORITHM FOR REFINING EXPERT SYSTEM KNOWLEDGE BASES
* Autho: ELLIS, BRET RANDALL
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: This research study promotes the concept that a self-adaptation
 algorithm could be designed and implemented to maintain an expert system
 knowledge base by modifying the current knowledge base and by allowing new
 knowledge to be added to the knowledge base without the aid of a knowledge
 engineer. A Self-Adapting FacIlity for Refining Expert Systems (SAFIRE) was
 designed, implemented, and tested. CCC, a diagnostic expert system designed
 to solve problems related to the use of WordPerfect 5.0 word processing
 software, and Hewlett Packard LaserJet Series II printers, was converted to
 a tabular format (CCC1). Using SAFIRE, CCC1 was able to add new rules,
 refine the sequence of questions, and modify the order of firing rules.
#####################
* Score: 831.8464223336346
* Doc #: 906
* Title: COMPUTER VISION IN MEDICINE: COLOR METRICS AND IMAGE SEGMENTATION METHODS FOR SKIN CANCER DIAGNOSIS
* Autho: UMBAUGH, SCOTT E.
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; COMPUTER SCIENCE;HEALTH SCIENCES, MEDICINE AND SURGERY; ARTIFICIALINTELLIGENCE; ENGINEERING, BIOMEDICAL
* Abstr: A front-end visual system for an expert system for skin tumor
 diagnosis will facilitate automatic detection of critical features,
 enabling the system to be operated with minimal user interaction. This
 research addresses the problem of using three-dimensional, color
 information for this purpose. A review of various color spaces and
 transforms that have appeared in the literature, along with a survey of
 general types of image segmentation techniques, is provided.
      The original color space was an RGB (red, green, blue) space created
 by digitizing color slides of skin tumor images using a monochrome video
 camera and three filters. Five hundred tumor slides were utilized for this
 research. Automatic induction software was successfully used to generate
 classification rules, based on color statistics only, for a set of 238
 tumor images. This set contained four possible diagnoses and success rates
 as high as 84% were achieved.
      Development of two algorithms for the segmentation of digitized skin
 tumor images based on color are presented. One segmentation algorithm is
 based on a spherical coordinate transform of the original RGB data, and the
 second color image segmentation method is based on the principal components
 transform of the color data in six different color spaces. It is shown that
 these color segmentation techniques are successful as aids in the
 identification of the following features: variegated coloring, tumor,
 crust, hair, scale, shiny (reflections), and ulcer. Variegated coloring was
 correctly identified with success rates as high as 92%. The segmentation
 success rates on the other features approach 95%. The successful results
 demonstrate the feasibility of the methodology presented to the development
 of a computer vision/expert system, and specifically the success of the
 color segmentation techniques, as well as the utility of automatic
 induction tools.
#####################
* Score: 831.8464223336346
* Doc #: 85
* Title: NEURAL NETWORK-BASED DECISION SUPPORT FOR INCOMPLETE DATABASES
* Autho: JIN, BO
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: The idea of applying neural network techniques to
 handling large databases containing incomplete
 information is presented. In an incomplete relational
 database model, maybe algebra operations are introduced
 to provide the user the opportunity to investigate the
 potential set of data values in order to draw his/her
 own conclusions. While such a set of operations allows a
 better information utilization, it can be the source of
 problems which should be taken into consideration. In a
 large database system, the data generated by maybe
 algebra operations can be enormous in size, erroneous in
 semantics, and less informative. To allow the maybe
 algebra operations to be practical, it is desired to
 develop a mechanism to increase the quality and
 integrity of the results, i.e., to filter out the
 erroneous and less informative data.
 A neural network-based decision support system is
 proposed to improve database performance in the presence
 of missing/incomplete information, provide intelligent
 assistance to the end-user, and explore the learning
 capability of a neural network-based system. The
 proposed system has a strong learning capability and
 adjusts itself according to the specific characteristics
 of the underlying database and the user special
 requirements. To support the automated training pair
 generation, the fuzzy logic technique is incorporated
 into the knowledge acquisition module to oversee, guide,
 and update the quality status of each training pattern.
 By approximate reasoning, the fuzzy controller works in
 a fashion that automatically incorporates the intrinsic
 ambiguities, imprecision, and undecidabilities
 associated with the quality status of each training
 pattern. Issues of enhancing the functionality of the
 decision support system are studied with a concentration
 on data representation for the decision-making network
 and multi-network configurations.
#####################
* Score: 831.8464223336346
* Doc #: 777
* Title: CAD/CAM-ON THE INTEGRATION OF CAPP/CACE FOR THE GENERATION OF LEAST COST MACHINING PROCESS PLAN FOR MULTIFEATURE WORK-PIECES
* Autho: PAI, CHENG CHA
* Desc : ENGINEERING, INDUSTRIAL
* Abstr: "Planning" has always been, and perhaps will always be, a decisive
 human trait. Recently, substantial accomplishments have been reported in
 making computers emulate this fundamental human characteristic. As a
 consequence, much technical improvement has also occurred in the area of
 Computer Aided Design (CAD), Computer Aided Process Planning (CAPP),
 Computer Aided Cost Estimating (CACE) and Computer Aided Manufacturing
 (CAM).
      There have been significant results in applying Artificial
 Intelligence (AI) techniques in automated process planning. The rules which
 human experts utilize to make decisions that involve the complex and
 non-quantifiable factors of machining are stored and used by an efficient
 CAPP system to automatically plan for the machining of parts. Despite these
 recent accomplishments, all CAPP systems developed to date are, at best,
 able to generate a low-cost machining sequence for only one form feature at
 a time.
      In this research, a model is developed for generating a process plan
 resulting in the least cost of machining of multi-feature work-pieces. A
 prototype demonstration system, Planner for Cost Control (PCC) is then
 presented and illustrated within the proposed framework.
      The decision rules utilized by human planners do not involve the
 detailed cost implications of decisions involved in all feasible processes
 associated with the sequencing of machining operations. Therefore, a
 knowledge-based CAPP system is developed in order to generate only feasible
 alternative machining operations. Detailed and cost-optimal cutting
 parameters are then quickly calculated by a CACE system. Further, these
 results are manipulated by an algorithm that accounts for tool movement and
 tool change costs and selects a low cost sequence of machining operations
 for the entire part. (Copies available exclusively from Micrographics
 Department, Doheny Library, USC, Los Angeles, CA 90089-0182.)
#####################
* Score: 831.8464223336346
* Doc #: 708
* Title: METHODS FOR INTELLIGENT ONLINE MONITORING AND DIAGNOSIS OF MANUFACTURING PROCESSES
* Autho: DU, RUXU
* Desc : ENGINEERING, MECHANICAL; COMPUTER SCIENCE; ARTIFICIALINTELLIGENCE
* Abstr: The study of on-line monitoring and diagnostics of manufacturing
 processes can be classified into four aspects: manufacturing processes,
 monitoring, diagnosis, and computer implementation. This thesis focuses on
 monitoring and diagnosis and develops four new methods: a monitoring
 method, a monitoring alarm threshold estimation method, and two diagnostic
 methods.
      The monitoring method is developed based on the dynamic data system
 (DDS) methodology. It uses the eigenvalue locations of the process
 auto-regressive moving average (ARMA) model and is equivalent to the
 damping ratio method but reduces the computational load significantly. This
 method is successfully applied in the tool wear monitoring of a milling
 process.
      The alarm threshold estimation method is a heuristic method developed
 based on two assumptions: (1) the given monitoring index can recognize the
 process conditions at any process working condition, and (2) the normalized
 variations of the index are about the same at different process working
 conditions. This method is tested in a spindle imbalance monitoring
 application.
      The key for the diagnosis is the relationship between process
 conditions and diagnostic indices. The first diagnostic method uses the
 fuzzy set theory to build a linear fuzzy equation to describe the
 relationship. By solving the fuzzy equation, a diagnostic decision can then
 be made. To demonstrate this method, an application in diagnosis of a
 tapping process is presented.
      The second diagnostic method originates from Artificial Intelligence
 (AI) and describes the relationship by a decision tree. It consists of
 three parts. First, a decision tree is built by applying the machine
 learning algorithm ID3. Second, the tree is pruned by the proposed tree
 modification method. Finally, a decision is made based on a new
 decision-making strategy. This method is applied to the same tapping
 process and to a welding process.
      This thesis also describes two computer implementations: (1) A
 multi-modular processors system (MMPS), which is a hierarchical computer
 system designed for on-line monitoring of a super die manufacturing cell.
 (2) A single-board computer implementation for on-line ARMA model parameter
 estimation.
#####################
* Score: 831.8464223336346
* Doc #: 597
* Title: DEVELOPMENT OF AN EXPERT SYSTEM FOR NONDESTRUCTIVE PAVEMENT STRUCTURAL EVALUATION
* Autho: CHOU, YEIN-JUIN
* Desc : ENGINEERING, CIVIL; ARTIFICIAL INTELLIGENCE
* Abstr: The use of nondestructive testing (NDT) in pavement structural
 evaluation is an important task in pavement engineering. The falling weight
 deflectometer has become the state-of-the-art NDT device. Many programs
 which use the mechanistic approach to obtain pavement layer moduli from
 measured surface deflection basin have been developed. However, the
 mechanistic approaches to solve the back-calculation problem have often met
 with the difficulty that proper assumptions based on engineering judgement
 are crucial to the degree of success that is achieved.
      The objectives of this study are: first, to provide guidelines for
 determining the structural properties of asphaltic concrete pavements,
 using NDT data, for use in pavement analysis, design, and rehabilitation;
 and second, to demonstrate the use of expert system techniques in assisting
 pavement structural evaluation. Specifically, a prototype expert system
 program named PASELS (for PAvement Structural EvaLuation System) is
 developed to provide a basin-by-basin analysis in determining the layer
 modulus when algorithmic backcalculation methods fail to produce reasonable
 results. The results of the PASELS system show that the expert knowledge
 and experience stored in the expert system can help to reduce the
 systematic errors in backcalculation, and a more reasonable estimation of
 the layer moduli may be obtained.
#####################
* Score: 831.8464223336346
* Doc #: 57
* Title: MODELING OF DISTILLATION COLUMN AND REACTOR DYNAMICS USING ARTIFICIAL NEURAL NETWORKS
* Autho: PARK, JUN-KU
* Desc : ENGINEERING, CHEMICAL; ARTIFICIAL INTELLIGENCE
* Abstr: Artificial neural networks (ANN's) have recently
 attracted much attention from the chemical engineers
 because of their potential advantages as follows: ANN's
 are inherently parallel and can learn nonlinear
 relations from sets of data. The application of ANN's to
 the modeling of dynamic chemical engineering processes
 has recently been discussed in several papers, however
 the applications were limited to some specifically
 selected examples. In this thesis, several typical
 chemical engineering processes are classified according
 to their degree of nonlinearity and dynamics, and
 representative examples in each of four classes are
 selected, i.e. distillation column, isothermal CSTR,
 nonisothermal CSTR, and batch reactor. The dynamic
 modeling of the selected processes using ANN's, mainly
 recurrent networks, is studied. Some basic research
 about the various parameters of ANN's is also discussed
 in the beginning of the thesis.
 As a result of basic research, some of the rules for the
 structural simplification of ANN's are verified. Noise
 filtering ability of ANN is also discussed. Several
 variations of the fully recurrent network for better
 applications are proposed. Through the study of
 application of ANN to a distillation column, the
 importance of considering the process dynamics is
 emphasized. Through the dynamic modeling of an
 isothermal CSTR using recurrent nets, the relation
 between the physical parameters and the ANN parameters
 are discussed. Various dynamic behaviors of a
 nonisothermal CSTR are modeled using recurrent nets, and
 a training strategy to overcome the convergence problem
 of ANN for difficult examples is proposed. Also a
 control method based on the developed ANN models is
 proposed. Finally, ANN model for the early on-line
 runaway prediction of a batch reactor is developed and
 its robustness to noise and process changes is
 demonstrated.
#####################
* Score: 831.8464223336346
* Doc #: 54
* Title: EVOLUTION OF EXPERT SYSTEMS
* Autho: PALACIOS CULEBRO, JOAQUIN MARCOS
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Expert systems are computer programs for providing
 expertise emulative of that which might be expected from
 human experts in solving complex problems for which
 analytical solutions are not available. Evolution of an
 expert system refers to the initial development of the
 system and its continuing modification in order to
 improve its performance. Any modifications made to an
 expert system have the potential of producing
 undesirable logical errors and side-effects that are
 difficult to find or prevent. Although much research has
 focused on facilitating the evolution of expert systems,
 most of the limitations still exist.
 This dissertation proposes an approach for structuring
 and evolving expert systems for applications in which
 the provision of the desired expertise is beyond the
 reach of either analytical or traditional heuristic
 approaches, but in which the knowledge domain is
 causally connected and the relevant causality can be
 expressed in procedural form.
 The research vehicle used is that of a hypothetical
 manufacturing system in which products of different
 types use some of the same workstations, and some of the
 product types loop back to workstations that they have
 previously used. The expertise sought is that of
 scheduling starts of products into the first stage of
 production so as to yield a stream of output that
 satisfies a user-specified balance among a variety of
 business performance measures including timeliness of
 production output.
 An approach for structuring and evolving expert systems
 is introduced. The main system elements are: A
 simulation model of the causal relations of the
 knowledge domain; a collection of heuristic algorithms,
 each guaranteed to produce a feasible solution to any
 problem lying within the scope of the system's
 expertise; and a module that determines, from user-
 supplied criteria, the best of the alternative
 solutions. The evolution approach is based on adding new
 heuristic algorithms rather than modifying existing
 ones.
 That approach is used to develop, and evolve through
 several steps of evolution, an expert system for the
 subject application. The experimental results assess the
 expert system's initial capability, and demonstrate the
 evolution of the system's expertise through addition of
 more heuristic algorithms.
#####################
* Score: 831.8464223336346
* Doc #: 4441
* Title: A DISTRIBUTED ARTIFICIAL INTELLIGENCE BASED SYSTEM FOR MANUFACTURING SYSTEM CONTROL IN SURFACE MOUNT PWB ASSEMBLY
* Autho: SHIH, WURONG
* Desc : ENGINEERING, INDUSTRIAL; ARTIFICIAL INTELLIGENCE
* Abstr: Surface mount Printed Wiring Board (PWB) assembly
 requires a variety of assembly processes and automated
 machinery to manufacture a range of PWB types.
 Throughput in PWB assembly can be improved by
 effectively exploiting the utilization and integration
 of the assembly equipment and processes. The global
 objective of this research was to exploit the
 integration and coordination that is possible in a
 surface mount PWB assembly facility through the use of
 DAI technology. The developed DAI prototype system helps
 to provide real-time responses to manufacturing related
 problems. This DAI based system allows for the design
 and implementation of an efficient mechanism to deal
 with dynamic production and process planning for PWB
 assembly.
 DAI technology in the manufacturing domain, especially
 in the field of surface mount technology, has not been
 widely explored. This research, which focuses on a DAI
 framework to solve production planning problems by using
 multiple expert systems, takes the first step in this
 direction. This system is unique as it not only utilizes
 the data transmission techniques existing in the
 traditional distributed manufacturing systems approach
 but also incorporates multiple knowledge based systems
 that work together to solve problems in the domain of
 interest.
 The prototype DAI system utilizes a group of problem
 solvers (intelligent agents) to form a problem solving
 team which implements dynamic production planning and
 production control in the problem domain. This prototype
 system was developed and implemented on a Local Area
 Network (LAN) where Personal Computers (PCS) are
 connected using bus topology. Each PC is considered as
 an intelligent agent in the DAI system. The DAI system
 was developed using Object Oriented Programming (OOP)
 concept which consider each intelligent agent to be an
 object in the software. The overall implementation of
 the system was then achieved by the manipulation of the
 individual objects/agents. This design concept allows
 further enhancement of the system by adding new objects
 (agents) to the system. An OOP development tool, Borland
 C++, was used for encoding all the reasoning,
 calculation, and communication processes required by the
 DAI system. This development tool incorporates the
 Microsoft Windows graphical user interface, thus
 providing a user friendly environment.
 The prototype system consists of six intelligent agents
 which are used to implement the dynamic production
 planning task. The planning task is divided into sub-
 tasks which can be solved by the individual intelligent
 agents with respect to their knowledge domains. The
 communication and cooperation between the intelligent
 agents is achieved by exchanging the messages and
 information within the blackboard communication
 framework. To resolve the conflict between various
 knowledge domains of the intelligent agents, a fuzzy
 coordination technique was used to negotiate the various
 solutions generated by the intelligent agents. This
 distributed processing capability allows intelligent
 agents to solve problems independently and concurrently,
 simplifying complex problems and providing instant
 decision support. (Abstract shortened by UMI.)
#####################
* Score: 831.8464223336346
* Doc #: 4368
* Title: LIGHT ADAPTATION AND SENSITIVITY CHANGE PROCESSES IN THE HUMAN RETINA: INVESTIGATING OPTO-ELECTRONIC APPLICATIONS
* Autho: WYSOKINSKI, TOMASZ WALDEMAR
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; ARTIFICIAL INTELLIGENCE; BIOLOGY, NEUROSCIENCE
* Abstr: Real-world image acquisition systems, such as the charge
 coupled device with frame grabber and computer, are
 still very limited in overall performance compared with
 the biological visual systems. The work done in this
 thesis originates from the fact that the solutions
 implemented by nature are both elegant and efficient,
 and that they should be the inspiration for designers to
 enhance the performance of artificial visual systems.
 In the first part of this research, we investigate the
 phenomena of light adaptation and sensitivity change
 processes that allow an eye to operate over 12 decades
 of light intensity range. Different adaptation
 mechanisms are scrutinised, and a model of those
 mechanisms is developed. Next we investigate how the
 visual system copes with light intensities beyond its
 dynamic range when glare occurs. The signal saturation
 in glare phenomena is used to reveal the highly complex
 structure of the retina. By applying a developed model
 of signal propagation in the Outer Plexiform Layer and
 by relating it to the experimental psychophysical data,
 we obtain a formula for the density of horizontal cell
 connections along the diameter of the human retina.
 By studying the architecture, neurology and physiology
 of the retina, we develop an equivalent Adaptive
 Filter/Photosensitive Device Array system capable of
 coping with a wide range of light conditions. The
 Adaptive Filter mimics some adaptation mechanisms of the
 human eye and offers the possibility of extending the
 dynamic range of operation of any image-acquiring system
 without affecting the electronic circuitry of the photo-
 sensor array.
 In the third part, we describe efforts to build one of
 the possible implementations of the adaptive filter,
 namely an optically addressed Photochromic Adaptive
 Filter (PAF), which utilizes photochromic material
 embedded in a polymeric environment. We successfully
 test the concept of extending the operating range of the
 constructed image-acquiring system by using optically-
 addressed PAF's. The system is modified to determine a
 Transformation Function of these filters using a simple,
 fast and easy-to-automate method.
 In the appendix we discuss several problems related to
 this research program. We estimate the measurement
 errors in detail. We also present a new method of
 preparing thin-layer silver-halide film and describe
 other possible implementations of the adaptive filter
 using active opto-electronic devices.
#####################
* Score: 831.8464223336346
* Doc #: 4362
* Title: ON MODELS OF TEACHING AND GEOMETRIC LEARNING
* Autho: MATHIAS, H. DAVID
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: In artificial intelligence, machines that learn are a
 long-sought goal. Computational learning theory is the
 formal study of methods by which machines learn, with
 emphasis on efficient use of time and data. A great
 difficulty confronted in learning theory research is
 appropriately modeling learning problems such that
 careful analysis of data and time requirements is
 possible and the underlying problem is adequately
 expressed. How should the learner acquire its
 information? How much interaction with the environment
 is allowed? What is the nature of the environment?
 We examine ways in which the environment with which the
 learner must interact may help the learner accomplish
 its task. Such models of teaching sharply contrast
 models of learning in which it is assumed that the
 environment is hostile. For many "real-world" learning
 problems, hostile environments may not be realistic.
 Some problems may be better modeled using a helpful
 teacher with whom the learner may interact. This may
 significantly reduce the learner's use of resources.
 We present the first formal teaching model, within the
 learning theory community, that excludes most "unnatural
 communication" between the teacher and the learner. We
 then improve this model by allowing bi-directional
 communication. The addition of interactivity appears to
 increase the power of the model: DNF formulas are
 teachable in this model while their teachability in the
 first model is open.
 In addition to models of teaching, we present algorithms
 for a general geometric class in established learning
 models. The class we consider consists of all labelings
 of the regions created by s, $(d-1)$-dimensional
 hyperplanes in d-dimensional space. We give an algorithm
 for this class, in the query learning model, when the
 slopes of the hyperplanes are known to the learner. In
 the PAC model of Valiant, we present an algorithm for
 this class without the restriction to known slopes. This
 is the most general geometric class for which a learning
 algorithm is known.
 Finally, we perform experiments to gauge the utility of
 models of teaching for real-world problems. We have
 implemented a simulator for the problem of robot terrain
 acquisition. We have obtained empirical results
 indicating that in this practical application helpful
 teachers are indeed useful.
#####################
* Score: 831.8464223336346
* Doc #: 4323
* Title: PERSEUS: AN EXTENSIBLE VISION SYSTEM FOR HUMAN-MACHINE INTERACTION
* Autho: KAHN, ROGER EWING
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Interpersonal communication involves more than simply
 spoken information. Gestures are commonly used to more
 efficiently and precisely communicate. An important
 gesture because of its descriptive power and frequency
 of use is pointing. To produce a more natural and
 powerful human-robot interface, a purposive visual
 architecture called Perseus has been developed and used
 to locate objects a person is pointing to.
 In real-time, Perseus is able to determine when a person
 enters the scene, track the relevant parts of the person
 including the hands and head, and recognize when he is
 pointing. Once the person points, the object pointed to
 is located. The Perseus architecture allows knowledge
 about the task and context to be used at all levels of
 visual analysis for improved performance. This knowledge
 is explicitly represented in the Perseus system to
 facilitate the extension of Perseus to other tasks and
 environments.
 This thesis describes Perseus and how it is used to
 solve this task. Experiments showing the success of the
 Perseus system with numerous naive users in varied
 environments is presented.
#####################
* Score: 831.8464223336346
* Doc #: 4286
* Title: SPEECH RECOGNITION BASED ON TIME-DELAY NEURAL NETWORKS
* Autho: WU, DUANPEI
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; ARTIFICIAL INTELLIGENCE
* Abstr: Two novel neural network architectures, Tunable Time-
 Delay Neural Networks with Signal-Shifting (TTDNN-SS)
 and K-Subspaces Time-Delay Autoassociators (KS-TDAA), as
 well as their training algorithms for isolated word and
 phoneme recognition, are presented. The first
 architecture is proposed to remove the constraint
 imposed on TDNNs that input patterns must have a fixed
 number of frames. Hence, it facilitates the application
 of TDNNs to isolated-word speech recognition. The TTDNN-
 SS architecture consists of a group of sub-nets, named
 as TDNN-SS. Each TDNN-SS is constructed with the basic
 architecture of TDNNs with signals shifting into the
 architecture and assigned to one and only one recognized
 unit, such as an isolated-word and phoneme. TTDNN-SS has
 successfuly been applied to isolated-word and phoneme
 recognition tasks with excellent recognition accuracies.
 For isolated-word recognition using the TI 20-word
 database, it yielded a recognition accuracy of 100% for
 all single-speaker data sets, 99.5% for a double-speaker
 data set and 98.83% for a six-speaker data set. For
 phoneme recognition, it obtained a recognition accuracy
 of 90.26% for a three voiced-stop-consonants (/b/, /d/
 and /g/), speaker-independent phoneme recognition task.
 The second proposed architecture, KS-TDAA, combines the
 time-delay design in TDNNs for phoneme recognition and
 the technique of Multi-Layer Perceptron (MLP)
 autoassociators. The KS-TDAA is designed to extend the
 approach of using autoassociators to perform phoneme
 recognition proposed by Nakamura et al. by incorporating
 time-delay units to input and hidden layers and using K
 such autoassociators to characterize the multi-model
 data. To train the KS-TDAA, a new training algorithm is
 also proposed. This non-discrimination training
 algorithm provides a method to avoid the drawback
 encountered in most neural networks of output values of
 networks not representing the candidate likelihoods.
 Simulations have shown that the KS-TDAA has
 significantly improved the recognition performance of
 Nakamura et al. approach. In the same time, it retains
 the same advantages such as network output values
 representing candidate likelihoods. Simulations have
 also shown that the performance of KS-TDAAs is very
 close to that obtained from successful neural network
 structures for phoneme recognition such as TDNNs and ST-
 LVQ.
#####################
* Score: 831.8464223336346
* Doc #: 4101
* Title: A DISTRIBUTED COOPERATIVE HOMOGENEOUS MULTI-AGENT APPROACH FOR PARALLEL FUZZY EXPERT SYSTEMS IN SURFACE MOUNT PWB ASSEMBLY
* Autho: CHU, HAI-CHENG ERIC
* Desc : ENGINEERING, INDUSTRIAL; COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Artificial Intelligence (AI) has been (and is being)
 used in many fields, especially in the manufacturing
 arena. Expert systems are a facet of AI which have been
 widely applied in different domains from medical
 diagnosis to industrial manufacturing. Recently, a new
 branch of AI called Distributed Artificial Intelligence
 (DAI) has emerged. It preserves the characteristics of
 AI while demonstrating its distinct features. The use of
 DAI for manufacturing systems applications in the
 Printed Wiring Board (PWB) assembly arena is the focus
 of this research along with the use of methods to deal
 with issues that relate to the uncertainty that is
 inherent in manufacturing.
 The Distributed Cooperative Homogeneous Multi-agent
 (DCHM) based approach uses multiple expert systems
 (including fuzzy expert systems) for problem solving in
 the surface mount PWB manufacturing domain. This
 research combines Computer-Aided Design (CAD), dynamic
 Computer-Aided Process Planning (CAPP), Concurrent
 Engineering (CE), Design for Manufacturing (DFM),
 Relational Data Base Management Systems (RDBMS), Local
 Area Networks (LAN), DAI, distributed expert systems,
 fuzzy expert systems, distributed processing, and the
 management of uncertainties into one problem solving
 system. The DCHM system includes an information center,
 a stencil printing intelligent agent and a reflow
 soldering intelligent agent. Review of the incoming CAD
 design from a DFM perspective is done, and a cost
 evaluation can also be performed.
 Inputs to the DCHM system is primarily in the form of a
 CAD file of the PWB's design. Outputs include a review
 of the design from manufacturing perspectives as well as
 process plans for some PWB assembly process (stencil
 printing and reflow soldering). Communication,
 cooperation, and negotiation among all the intelligent
 agents with respect to the dynamic process planning
 function is reflected in the DCHM system. Object-
 Oriented Programming (OOP) concepts are widely used in
 this research. Personal computers act as intelligent
 agents, and the interaction between them occurs through
 communication across a LAN.
#####################
* Score: 831.8464223336346
* Doc #: 402
* Title: THE USE OF CONSTRUCT VALIDITY IN THE EVALUATION OF INTELLIGENT TUTORING SYSTEM PERFORMANCE
* Autho: SHOTSBERGER, PAUL GAHLEN
* Desc : EDUCATION, TESTS AND MEASUREMENTS; EDUCATION, TECHNOLOGY; EDUCATION, MATHEMATICS; ARTIFICIAL INTELLIGENCE
* Abstr: The National Council of Teachers of Mathematics (NCTM)
 has identified the use of computers as a necessary
 teaching tool for enhancing mathematical discourse in
 schools. Through the use of technology, NCTM envisions
 the transformation of classrooms into laboratories for
 experimentation and exploration, with the consequent
 altering of the teacher's role to that of a partner in
 and facilitator of student discovery. One possible
 vehicle of technological change in mathematics
 classrooms is the Intelligent Tutoring System (ITS), an
 artificially intelligent computer-based tutor. The focus
 of this dissertation is on the usefulness of construct
 validation methods for evaluating ITS performance.
 The pilot study analyzed on-line data from a classroom
 evaluation involving 74 students solving an algebra
 equation using The RAND Corporation Intelligent Tutor
 for Basic Algebra. Construct validation techniques
 provided a means of extracting factors, comparing these
 factors to hypothesized constructs for the system, and
 then supplying and testing rival hypotheses concerning
 the extracted factors' identities. The main study used a
 similar methodology to analyze data from another ITS,
 the ANGLE geometric proof tutor developed at Carnegie-
 Mellon University.
 The first phase of the main study employed data from a
 laboratory evaluation of 15 students using ANGLE to
 establish the consistency, and therefore the
 suitability, of the data for a construct validity study.
 The second phase used data from a classroom evaluation
 in which 42 students had solved a proof problem using
 ANGLE. Two factors were extracted, with one of the two
 factors being identified as a hypothesized construct of
 the system, with some contamination. A rival hypothesis
 was generated for the second factor, but the meaning of
 the factor could not be determined due to a low
 reliability coefficient for its measures.
 Results from these studies provided useful information
 concerning the strength of hypothesized constructs of
 the RAND ITBA and ANGLE, and this information served to
 amplify the results of previous evaluations carried out
 with the ITSs. Implications for the practice of ITS
 evaluation are provided, as well as directions for
 future research.
#####################
* Score: 831.8464223336346
* Doc #: 4001
* Title: LEARNING RESTRICTED-READ BRANCHING PROGRAMS WITH QUERIES
* Autho: WILKINS, DAWN ELISABETH
* Desc : COMPUTER SCIENCE; MATHEMATICS; ARTIFICIAL INTELLIGENCE
* Abstr: There are two predominant models of computational
 learning theory--the probably approximately correct
 (PAC) model and the exact model. The goal of
 computational learning theory is to develop algorithms
 for efficiently learning concept classes and to show
 that classes cannot be efficiently learned. Boolean
 classes are among the most natural classes to study and
 are the focus of this dissertation.
 There is cryptographic evidence to support the belief
 that the class of general branching programs is not
 predictable. Thus, our emphasis is on restricted
 subclasses of of branching programs. We present both
 positive and negative results. Specifically, a general
 framework for learning projection-closed, "augmentable"
 concept classes in the exact model (with membership
 queries) is developed. We then present a
 characterization of the class of $mu$-branching programs
 (where each variable may appear in the branching program
 at most once). The characterization is used to show that
 $mu$-branching programs are augmentable, and therefore
 learnable by the general framework. Using a result by
 Angluin, it is easy to show that $mu$-branching programs
 are also learnable in the PAC model (with membership
 queries). The negative results show that (i) $mu$-
 branching programs cannot be learned with membership
 queries alone, or equivalence queries alone, (ii) $0kmu$-
 branching programs (where each variable may appear at
 most k times in the representation), for $k ge 3$, are
 not predictable modulo cryptographic assumptions, and
 (iii) the class of read-k-times branching programs
 (where each variable may appear at most k times on any
 root-to-terminal path), for $k ge 2$, is not predictable
 modulo cryptographic assumptions.
 Also included in the dissertation is a chapter on
 equivalence testing of branching programs. Using the
 characterization of $mu$-branching programs, we show
 that a pair of $mu$-branching programs can be tested for
 equivalence in O($0nalpha(n$)) time where n is the
 number of nodes in the larger of the $mu$-branching
 programs. Equivalence testing for the classes of $0kmu$-
 branching programs, for $k ge 3$ and read-k-times
 branching programs, for $k ge 2$ are show to be co-NP-
 complete.
#####################
* Score: 831.8464223336346
* Doc #: 3963
* Title: A PROBABILISTIC ALTERNATIVE TO FUZZY LOGIC CONTROLLERS
* Autho: BARRETT, JOHN DOUGLAS
* Desc : STATISTICS; COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: The fuzzy logic controller has been used increasingly in
 industrial applications. Proponents of fuzzy logic have
 claimed that probability can not be used in these
 applications. In this dissertation, we show that
 probability can be used to construct a controller which
 can serve the same purpose as the fuzzy logic controller
 without sacrificing any performance qualities.
#####################
* Score: 831.8464223336346
* Doc #: 396
* Title: NEURAL NETWORKS IN FINANCIAL DISTRESS PREDICTION: AN APPLICATION TO THE LIFE INSURANCE INDUSTRY
* Autho: HUANG, CHIN-SHENG
* Desc : ECONOMICS, FINANCE; ARTIFICIAL INTELLIGENCE
* Abstr: In this dissertation, a multi-layered feedforward neural
 network trained with the genetic algorithm (ANN) is used
 to forecast financial distress in life insurers.
 Comparisons of prediction efficiency of ANN with three
 traditional statistical techniques, namely, k-nearest
 neighbor (k-N), Logit, and discriminant analysis (DA),
 are systematically examined across three insolvency
 models under different criteria.
 The underlying insolvency models are built up from a
 data base provided by the National Association of
 Insurance Commissioners (NAIC). Two models using the
 NAIC IRIS' ratios are created with one (HNR) strictly
 following NAIC truncating ratios and the another (TUR)
 with full measurements of IRIS ratios. The effect of
 NAIC calculation rules on prediction efficacy is tested.
 The third insolvency model (6-VARIABLE) is created by
 using stepwise discriminant selection from a broad range
 of 33 financial variables. Three criteria, namely,
 misclassification cost, resubstitution risk, and naive
 comparison, are set up to examine the prediction
 efficacy of ANN, k-N, Logit, and DA across models.
 The empirical results show that ANN consistently
 outperforms the three traditional methods for the full
 measurement, TUR, model, in out of sample prediction;
 Logit performs best using NAIC's truncated HNR model but
 is severely effected by the outliers in the TUR model; k-
 N and DA are less efficient in all models. The
 discriminant stepwise selection, 6-VARIABLE, model has
 less discriminant capacity than either IRIS models. This
 study suggests that more work is needed to effectively
 identify discriminant variables in insurance insolvency
 prediction using the promising area of dimension
 reduction by neural networks instead of using the
 traditional discriminant analysis selection procedure.
#####################
* Score: 831.8464223336346
* Doc #: 3945
* Title: CONTENT ADDRESSABLE NETWORKS
* Autho: BRODSKY, STEPHEN ANDREW
* Desc : COMPUTER SCIENCE; ENGINEERING, ELECTRONICS AND ELECTRICAL; PHYSICS, ELECTRONICS AND ELECTRICITY; ARTIFICIAL INTELLIGENCE
* Abstr: High performance neural network computational systems
 are investigated through theoretical and experimental
 analysis of a new family of algorithms, the Content
 Addressable Networks (CAN). The CAN algorithms are
 demonstrated to be significantly more effective
 implementations of neural networks in computer
 technology than current network models.
 Three CAN learning models have been developed:
 supervised, self-organized, and tutored. The models are
 described, derived, compared to leading neural networks,
 and proven to converge. Conditions are established for
 optimal recall. The networks are analyzed from a cost
 benefit perspective, quantitatively establishing the
 advantages of CAN networks over existing network
 implementations.
 Three experimental CAN systems have been constructed: An
 optoelectronic system demonstrates all optical
 computation of CAN weight recall, fault tolerance, and
 learning on the experimental system. A second
 optoelectronic system shows all optical computation of
 CAN weight learning and recall with additional levels of
 fault tolerance. A VLSI CAN chip illustrates the
 capabilities and efficiency of CAN in CMOS VLSI. A key
 circuit has been developed which dramatically reduces
 area and greatly increases computational speed of CAN
 networks.
#####################
* Score: 831.8464223336346
* Doc #: 3831
* Title: NICHING METHODS FOR GENETIC ALGORITHMS
* Autho: MAHFOUD, SAMIR W.
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Niching methods extend genetic algorithms to domains
 that require the location and maintenance of multiple
 solutions. Such domains include classification and
 machine learning, multimodal function optimization,
 multiobjective function optimization, and simulation of
 complex and adaptive systems.
 This study presents a comprehensive treatment of niching
 methods and the related topic of population diversity.
 Its purpose is to analyze existing niching methods and
 to design improved niching methods. To achieve this
 purpose, it first develops a general framework for the
 modelling of niching methods, and then applies this
 framework to construct models of individual niching
 methods, specifically crowding and sharing methods.
 Using a constructed model of crowding, this study
 determines why crowding methods over the last two
 decades have not made effective niching methods. A
 series of tests and design modifications results in the
 development of a highly effective form of crowding,
 called deterministic crowding. Further analysis of
 deterministic crowding focuses upon the distribution of
 population elements among niches, that arises from the
 combination of crossover and replacement selection.
 Interactions among niches are isolated and explained.
 The concept of crossover hillclimbing is introduced.
 Using constructed models of fitness sharing, this study
 derives lower bounds on the population size required to
 maintain, with probability $gamma$, a fixed number of
 desired niches. It also derives expressions for the
 expected time to disappearance of a desired niche, and
 relates disappearance time to population size. Models
 are presented of sharing under selection, and sharing
 under both selection and crossover. Some models assume
 that all niches are equivalent with respect to fitness.
 Others allow niches to differ with respect to fitness.
 Focusing on the differences between parallel and
 sequential niching methods, this study compares and
 further examines four niching methods--crowding,
 sharing, sequential niching, and parallel hillclimbing.
 The four niching methods undergo rigorous testing on
 optimization and classification problems of increasing
 difficulty; a new niching-based technique is introduced
 that extends genetic algorithms to classification
 problems.
#####################
* Score: 831.8464223336346
* Doc #: 364
* Title: SEARLE'S CHINESE BOX: THE CHINESE ROOM ARGUMENT AND ARTIFICIAL INTELLIGENCE.
* Autho: HAUSER, LARRY STEVEN
* Desc : PHILOSOPHY; COMPUTER SCIENCE; PSYCHOLOGY, GENERAL; ARTIFICIAL INTELLIGENCE
* Abstr: The apparently intelligent doings of computers occasion
 philosophical debate about artificial intelligence (AI).
 Evidence of AI (such doings) is not bad; arguments
 against AI are: such is the case for. One argument
 against AI--currently, perhaps, the most influential--is
 considered in detail: John Searle's Chinese room
 argument (CRA). This argument and its attendant thought
 experiment (CRE) are shown to be unavailing against
 claims (of AI proper) that computers can and even do
 think. CRA is formally invalid and informally
 fallacious. CRE's putative experimental result is not
 robust (similar "experiments" give conflicting
 results) and fails to generalize from understanding to
 other mental attributes as claimed. Further, CRE depends
 for its credibility, in the first place, on a dubious
 tender of the epistemic privilege of overriding all
 "external" behavioral evidence to first person
 disavowals of mental properties like understanding.
 Advertised as effective against AI, Searle's argument is
 an ignoratio elenchi, feigning to refute AI by disputing
 a similar (but logically independent) claim of
 "strong AI" or Turing machine functionalism
 (FUN) metaphysically identifying minds with programs.
 AI, however, is warranted independently of FUN: even if
 CRA disproved FUN this would still fail to refute or
 seriously disconfirm claims of AI. Searle's contention
 that everyday predications of mental terms of computers
 are discountable as equivocal (figurative) "as-
 if" predications--impugning independent seeming-
 evidence of AI if tenable--is unwarranted. Lacking
 intuitive basis, such accusations of ambiguity require
 theoretical support. The would-be theoretical
 differentiation of intrinsic intentionality (ours) from
 as-if intentionality (theirs) Searle propounds to
 buttress allegations of ambiguity against mental
 attributions to computers, however, depends either on
 dubious doctrines of objective intrinsicality according
 to which meanings are physically in the head or on even
 more dubious (as if dualistic) notions of subjective
 intrinsicality according to which meanings are
 phenomenologically "in" consciousness. Neither
 would such would-be differentiae as these
 unproblematically rule out seeming instances of AI if
 granted. The dubiousness of as if dualistic
 identification of thought with consciousness also
 undermines the epistemic privileging of the "first
 person point of view" crucial to Searle's thought
 experiment.
#####################
* Score: 831.8464223336346
* Doc #: 3600
* Title: A SOFTWARE ENGINEERING APPROACH TO THE DEVELOPMENT OF FUZZY CONTROL SYSTEMS
* Autho: ISOMURSU, ESA PEKKA
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE 2000, FIN-02044  VTT, FINLAND
* Abstr: We examine the development process of fuzzy control
 software from the software engineering point of view. We
 propose an approach to the development of fuzzy logic
 controllers (FLCs) that makes their industrial
 exploitation easy and efficient. We base our approach on
 the assumption that the development process as well as
 its outcome should be intelligible to the human experts
 who create or use the FLC.
 The proposed approach consists of three views of the FLC
 software development process: development model,
 development methods and tools, and design parameters.
 For each view, methodology is proposed and constructed
 that helps the development of FLCs for industrial use.
 This methodology includes a detailed model of the FLC
 software development process, methods for the tuning of
 FLCs, a development tool, and designs of FLCs that make
 their structure intelligible.
 Using the proposed methodology we have managed to
 successfully implement various industrial FLCs. Some of
 them are discussed in detail in this thesis.
#####################
* Score: 831.8464223336346
* Doc #: 3509
* Title: AN INDUCTIVE APPROACH TO THE EXTRACTION OF ROADS FROM MULTISPECTRAL SATELLITE IMAGES
* Autho: MARIN, JOHN ANTHONY
* Desc : ENGINEERING, SYSTEM SCIENCE; REMOTE SENSING; ARTIFICIAL INTELLIGENCE
* Abstr: Extracting roads from satellite images is an important
 problem with applications in both the military and
 private sectors. Manually extracting roads is a time-
 consuming and tedious task, requiring numerous man-hours
 to process an image, and is usually limited to single-
 dimensional data. This dissertation presents a two-step
 inductive method for extracting rural roads in noisy,
 multispectral, satellite images. The procedure is
 referred to as inductive because the system infers the
 general characteristics of a road from a set of training
 examples.
 In the first step, a neurally inspired classifier,
 Learning Vector Quantization (LVQ), is employed to
 initially classify image pixels. This first step
 includes an original pixel allocation algorithm that
 assigns pixels to training matrices and pre-positions a
 set of reference vectors. Also, a modification to the
 LVQ process is presented that accounts for patterns not
 represented by exemplars in the training matrices.
 The second step describes a process for tracking roads
 in a binary image, such as the output from the LVQ
 classification procedure. In the first phase of the
 tracking process, a new noise reduction algorithm is
 introduced and justified. In the second phase of the
 tracking process, segments are traced and selected road
 segments are linked using a potential function-guided
 best-first search. This technique allows for the
 assimilation of supporting information, as evidenced by
 the inclusion of data from a Digital Elevation Model.
 The system presented in this research differs from
 existing systems in that it is based-on the spectral
 properties of roads rather than locating edges in an
 image, and is specifically designed for multidimensional
 data. The system described here, unlike existing
 approaches, represents a complete road extraction
 procedure that goes from a satellite image to a traced
 road. The system was tested on a SPOT satellite image of
 Albemarle county, Virginia, and the results are
 presented and discussed.
#####################
* Score: 831.8464223336346
* Doc #: 3496
* Title: BOOTSTRAP BASED COOPERATIVE PROCESSES IN COMPUTER VISION
* Autho: CHO, KYUJIN
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; ARTIFICIAL INTELLIGENCE
* Abstr: A new approach for executing computer vision tasks is
 presented. In real situations the complexity of the
 input data and/or computational procedure limits the
 possibilities of rigorous modeling, and therefore it is
 difficult to design algorithms optimal for a wide
 variety of operating conditions. Validating the
 assumptions embedded into a computer vision algorithm
 for the given input is a necessary condition if robust
 techniques are desired.
 We propose the use of bootstrap based cooperative
 processes for validation. The set of outputs, obtained
 by perturbing the input data in the execution of the
 task, defines the empirical distribution of the output.
 From the distribution an output confidence measure under
 the given operating conditions can then be assessed.
 Based on these confidence values the task can be
 executed using less constraining assumptions about the
 data and thus improving the robustness of any algorithm.
 The derived confidences also provide tools for
 evaluating the performance of the system under realistic
 operating conditions.
 The proposed approach is motivated by resampling
 techniques developed in statistics during the past
 decade, especially the bootstrap. The bootstrap method
 is a nonparametric estimation technique of the
 statistical behavior of an estimate when only a single
 sample of the input data is available. We make extensive
 use of bootstrap techniques. The methodology of using
 cooperative processes is first applied to evaluate and
 compare the performance of several edge detection
 systems. Confidences are obtained by using the
 perturbation of nuisance properties of the input,
 properties with no relevance for the output under ideal
 conditions. Based on the confidence values, an edgemap
 independent of the gradient magnitude is derived, As
 another example we show that robust image segmentation
 can be achieved based on the consensus information
 extracted from the output of several region-adjacency-
 graph (RAG) pyramids having a probabilistic component.
 The generality of the new technique is discussed and its
 applications for other computer vision tasks are
 proposed as further research.
#####################
* Score: 831.8464223336346
* Doc #: 3396
* Title: A FRAMEWORK FOR THE DESIGN OF A VOICE-ACTIVATED, INTELLIGENT, AND HYPERMEDIA-BASED AIRCRAFT MAINTENANCE MANUAL
* Autho: PATANKAR, MANOJ SHASHIKANT
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE; ENGINEERING, AEROSPACE; ENGINEERING, ELECTRONICS AND ELECTRICAL
* Abstr: Federal Aviation Regulations require Aviation
 Maintenance Technicians (AMTs) to refer to approved
 maintenance manuals when performing maintenance on
 airworthy aircraft. Because these manuals are paper-
 based, larger the size of the aircraft, more cumbersome
 are the manuals. Federal Aviation Administration (FAA)
 recognized the difficulties associated with the use of
 large manuals and conducted studies on the use of
 electronic media as an alternative to the traditional
 paper format. However, these techniques do not employ
 any artificial intelligence technologies and the user
 interface is limited to either a keyboard or a stylus
 pen. The primary emphasis of this research was to design
 a generic framework that would allow future development
 of voice-activated, intelligent, and hypermedia-based
 aircraft maintenance manuals. A prototype (VIHAMS--Voice-
 activated, Intelligent, and Hypermedia-based Aircraft
 Maintenance System) was developed, as a secondary
 emphasis, using the design and development techniques
 that evolved from this research.
 An evolutionary software design approach was used to
 design the proposed framework and the structured rapid
 prototyping technique was used to produce the VIHAMS
 prototype. VoiceAssist by Creative Labs was used to
 provide the voice interface so that the users (AMTs)
 could keep their hands free to work on the aircraft
 while maintaining complete control over the computer
 through discrete voice commands. KnowledgePro for
 Windows $rmsp0TM,$ an expert system shell, provided
 "intelligence" to the prototype. As a result of this
 intelligence, the system provided expert guidance to the
 user. The core information contained in conventional
 manuals was available in a hypermedia format. The
 prototype's operating hardware included a notebook
 computer with a fully functional audio system. An
 external microphone and the built-in speaker served as
 the input and output devices (along with the color
 monitor), respectively.
 Federal Aviation Administration estimates the United
 States air carriers to operate 3,991 large jet aircraft
 in the year 1996 (FAA Aviation Forecasts, 1987-1998).
 With an estimate of seventy manuals per such aircraft,
 the development of intelligent manuals is expected to
 impact 279,370 manuals in this country. Soon, over 55
 thousand maintenance technicians will be able to carry
 the seven pound system to an aircraft, use voice
 commands to access the aircraft's files on the system,
 seek assistance from the expert system to diagnose the
 fault, and obtain instructions on how to rectify the
 fault.
 The evolutionary design approach and the rapid
 prototyping techniques were very well suited for the
 spiral testing strategy. Therefore, this strategy was
 used to test the structural and functional validity of
 this research. Professors Darrell Anderson and Brian
 Stout (Aviation faculty at San Jose State University)
 and Mr. Gregory Shea (a United Airlines mechanic and
 SJSU student) are representatives of the real-world
 users of the final product. Therefore, they conducted
 the alpha test of this prototype. Mr. Daniel Neal and
 Mr. Stephen Harms have been actively involved in light
 aircraft maintenance for more than ten years. They
 evaluated the prototype's usability. All the above
 evaluators used standard testing tools and evaluated the
 prototype under field conditions.
 The evaluators concluded that the VIHAMS prototype used
 a valid fault diagnosis strategy, the system
 architecture could be used to develop similar systems
 using off-the-shelf tools, and the voice input system
 could be refined to improve its usability.
#####################
* Score: 831.8464223336346
* Doc #: 2963
* Title: A HYBRID APPROACH FOR CLASSIFICATION LEARNING FROM DATABASES
* Autho: LEE, CHANGHWAN
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: By examining the content of a database, interesting
 relationships among data can be discovered
 automatically. The extracted knowledge can facilitate
 classification and/or reasoning in databases. The
 purpose of my research is to set forth a basic theory
 for a classification learning system from databases, and
 information theory serves as the theoretical background
 of the entire system.
 The approach employed in this research is composed of
 three phases: pre-processing phase, rule-based learning
 phase, and similarity-based learning phase. In the
 preprocessing phase, numeric attributes are discretized
 with varying intervals based on the amount of
 information they contain. A new discretization method,
 called context-sensitive discretization, which minimizes
 the information loss during discretization is proposed.
 While most machine learning research has been primarily
 concerned with the development of systems that implement
 one type of learning strategy, in this research, we use
 a multistrategy approach which combines rule-based
 learning with similarity-based learning, and show how
 this marriage allows for overall better performance.
 In the rule-based learning phase, for each rule, we
 derive an entropy function, based on Hellinger
 divergence, which can measure the amount of information
 each inductive rule contains. We show how well the
 Hellinger divergence measures the importance of each
 rule, and also propose some heuristics to reduce the
 computational complexity by analyzing the
 characteristics of the Hellinger measure. In the
 similarity-based learning phase, we improve the current
 similarity-based learning method in a number of ways. An
 unified form of similarity measure, applicable to every
 type of database attribute, is proposed, and the weight
 of each attribute is calculated automatically.
 The system has been implemented and tested intensively
 on a number of well-known machine learning data sets.
 The performance of the system has been compared with
 that of other well-known classification learning
 techniques. The results presented in this dissertation
 support the thesis that hybrid algorithms combining rule-
 based classification and similarity-based classification
 can be tractable, that resulting rules can capture
 complex interrelationships among attributes in a variety
 of databases, and that these combined methods can thus
 classify new cases with high accuracy.
#####################
* Score: 831.8464223336346
* Doc #: 2688
* Title: APPROXIMATION ALGORITHMS AND COMPLEXITY RESULTS FOR MACHINE LEARNING
* Autho: LIN, JYH-HAN
* Desc : COMPUTER SCIENCE; OPERATIONS RESEARCH; STATISTICS;ARTIFICIAL INTELLIGENCE
* Abstr: Real-world learning problems are often characterized by
 high-dimensional input and output spaces, ever changing learning
 environments, and the need to form and evaluate hypotheses quickly. The
 requirement for fast learning and real-time computation leads us to
 consider three computational models with inherently massive
 parallelism--the PRAM model for parallel computing, neural nets, and
 memory-based learning systems. A common thread of this thesis is Valiant's
 model of concept learning from examples, which combines the fields of
 artificial intelligence and complexity theory and provides us with a
 framework for characterizing learnable concept classes.
      A parallel learning model based upon PRAMs gives us insights regarding
 the fundamental computational limits on learning using parallel processors.
 We examine quantitatively what we can gain by using parallelism to learn
 concepts from examples.
      Artificial neural nets are computational models with enormous
 potential for parallelism. Our neural net learning model encapsulates the
 idea of modular neural nets. Our results indicate that, without any
 domain-specific knowledge, the training problem is in general intractable.
 On the other hand, if the concept class to be learned is known a priori and
 the net architecture is appropriately sized and properly interconnected,
 sometimes the training problem can be much easier.
      A memory-based learning system is an extended memory management system
 for "remembering" experience. We propose a model for memory-based learning
 and use it to analyze several methods for learning smooth functions by
 memory-based learning systems. We derive the sample size and system
 complexity for each method. The problem of clustering is identified as a
 central issue for memory-based learning, thus motivating the development of
 clustering algorithms with provably worst-case performance guarantees.
      Several approximation algorithms for memory-based learning in this
 thesis are based upon new randomized and deterministic methods for
 transforming an optimal solution of a relaxed integer linear program into a
 provably good solution for the corresponding discrete optimization problem.
 Our techniques lead to the first known approximation algorithms with
 provable performance guarantees for the s-median problem, the generalized
 assignment problem, and the tree pruning problem. Besides machine learning,
 the approximation algorithms developed in this thesis have important
 applications to data compression, vector quantization, computer graphics,
 image processing, clustering, regression, network location, scheduling, and
 communication. We report some experimental results on lossy image
 compression and network location, thus demonstrating the practical
 importance of our methods.
#####################
* Score: 831.8464223336346
* Doc #: 2405
* Title: EXTRACTION AND USE OF CONTEXTUAL ATTRIBUTES FOR THEORY COMPLETION: AN INTEGRATION OF EXPLANATION-BASED AND SIMILARITY-BASED LEARNING
* Autho: DANYLUK, ANDREA POHORECKYJ
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: This research investigates the use of contextual cues to address
 problems in machine learning that arise from assumptions about the initial
 knowledge that is necessary for the acquisition of new information. Machine
 learning approaches may be placed along a spectrum describing purely
 inductive to purely deductive techniques. Inductive systems possess
 essentially no explicit knowledge that can be used in acquiring new facts,
 while deductive systems are assumed to contain a complete theory of the
 domain. Most work in machine learning has concentrated on approaches at the
 two ends of the spectrum. This dissertation describes an approach that
 integrates inductive and deductive methods. It provides a mechanism by
 which induction can be used in order to detect and acquire knowledge
 missing from the domain theory of a deductive system, while extracting
 information to guide the inductive search for missing knowledge. This
 information may be termed contextual.
      Much knowledge can be brought to bear in filling gaps in a deductive
 system's domain theory. This might include information about the domain
 theory itself or perhaps the history of the learning system. I show that a
 domain-independent set of attributes can be used to describe contextual
 information available in the deductions made by a system. Furthermore, I
 present an algorithm for using that information to generate
 domain-independent heuristics that guide induction. The heuristics use
 context information to select examples for induction of missing information
 and guide the way the examples are interpreted. My empirical investigations
 have yielded a characterization of the efficacy of various subsets of
 attributes.
      The algorithm for extracting and applying contextual knowledge is
 implemented in the Gemini system. Gemini combines similarity-based
 learning, an inductive technique, and explanation-based learning, a
 deductive method, in an architecture of mutual dependence. Similarity-based
 learning is used to fill gaps detected in the domain theory used by
 explanation-based learning. I have developed Gemini as a general learning
 system and have tested it in four domains. It has been the testbed for my
 investigation of contextual attributes and confirms that there exists a set
 of domain-independent attributes that can effectively guide an inductive
 search for concept definitions.
#####################
* Score: 831.8464223336346
* Doc #: 2296
* Title: IN-PROCESS MACHINE DEGRADATION MONITORING AND FAULT DETECTION USING A NEURAL NETWORK APPROACH
* Autho: LEE, JAY
* Desc : ENGINEERING, MECHANICAL; ARTIFICIAL INTELLIGENCE;ENGINEERING, ELECTRONICS AND ELECTRICAL
* Abstr: Machines degrade as a result of aging and wear, which decreases
 performance reliability and increases the potential for faults and
 failures. The impact of machine faults and failures on factory productivity
 is an important concern for manufacturing industries. Economic impacts
 relating to machine availability and reliability, as well as corrective
 (reactive) maintenance costs, have prompted facilities and factories to
 improve their maintenance techniques and operations to monitor machine
 degradation and detect faults. This dissertation presents an innovative
 methodology that can change maintenance practice from that of reacting to
 breakdowns, to one of preventing breakdowns, thereby reducing maintenance
 costs and improving productivity.
      The author proposes a proactive (integrated predictive and preventive)
 maintenance technique that uses a neural network to monitor machine
 degradation and detect faults. The objective of this research was to
 develop a in-process behavioral learning, monitoring, and recognition
 methodology that can be used to monitor the degradation of machines
 (including sensors, actuators, and other components) and detect faults in a
 manufacturing system. A mathematical model of degradation based on a Markov
 approach was developed to investigate the degradation process and its
 implications. The study has shown that the failure probability is a
 function of the degradation probability (conditional probability). To
 monitor the machine behavior adaptively, a pattern discrimination model
 (PDM) based on a cerebellar model articulation controller (CMAC) was
 developed, investigated, and tested through simulation and experimentation.
 The simulation results have shown that the machine behavioral pattern can
 be measured quantitatively. Two test examples (encoder position monitoring
 and robot performance monitoring) were used to study the feasibility of the
 developed technique. Experimental results have shown that the developed
 technique can monitor machine degradation and detect faults adaptively.
      The potential application of this research is in the development of a
 software tool set which could enable the rapid development of system
 monitoring and fault detection software for production equipment. This
 methodology could help operators set up machines for a given criterion,
 determine whether the machine is running correctly, and predict problems
 before they occur. As a result, maintenance hours could be used more
 effectively and productively.
#####################
* Score: 831.8464223336346
* Doc #: 1989
* Title: A KNOWLEDGE REPRESENTATION METHODOLOGY FOR THE DEVELOPMENT OF KNOWLEDGE-BASED SYSTEMS FOR THE DETECTION AND DIAGNOSIS OF CANCER
* Autho: NANCE, KARA L.
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE; ENGINEERING,BIOMEDICAL; HEALTH SCIENCES, MEDICINE AND SURGERY
* Abstr: Current artificial intelligence research includes expanding the
 problem domain while maintaining power capabilities. There exist expert
 systems which detect or diagnose specific types of cancer. Ideally, this
 domain could be expanded to excompass a wider range of cancer types.
      Similarities in different cancers were documented in an attempt to
 find a knowledge representation methodology which would support domain
 expansion. The observed similarities in the cancer staging systems were
 compared with the knowledge representation methodologies used in the
 implementation of some current cancer expert systems. It was then
 determined that domain expansion could best be achieved by devoloping a new
 knowledge representation methodology which supports the standard staging
 systems used in cancer diagnosis.
      The new cancer representation methodology is a frame-based semantic
 network developed specifically for the cancer domain although future
 applications are widespread. The system uses four types of links and four
 types of entities developed to take advantage of the similarities in the
 different cancer types. The links and entities meld together to form a
 knowledge representation methodology suitable for a range of cancers. The
 new knowledge representation methodology exhibits the desirable properties
 of a frame-based semantic network and some new considerations which have
 not yet been readily implemented in other knowledge-based systems. The
 system demonstrates improvements over existing systems which approach
 similar problems and represents a significant step in the construction of a
 knowledge-based expert system for a group of cancers.
#####################
* Score: 831.8464223336346
* Doc #: 197
* Title: OPTIMAL TERMINAL CONTROL USING FEEDFORWARD NEURAL NETWORKS
* Autho: PLUMER, EDWARD STANLEY
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; ARTIFICIAL INTELLIGENCE
* Abstr: Multi-layer, sigmoidal neural networks are capable of
 approximating continuous, multi-variate functions and,
 as such, can implement nonlinear state-feedback
 controllers. The most common algorithm for training such
 controllers is backpropagation through time (BPTT).
 However, BPTT does not deal with terminal control
 problems in which the cost function includes the elapsed
 trajectory-time.
 This difficulty is investigated in the first half of the
 thesis. The controller design is reformulated as an
 optimization problem defined over the entire field of
 extremals with the set of trajectory times incorporated
 into this cost function. Necessary first-order
 stationary conditions are derived which correspond to
 standard BPTT with the addition of certain
 transversality conditions. The new gradient algorithm
 based on these conditions is called time-optimal
 backpropagation through time (TOBPTT). This approach is
 tested on several open final-time problems with very
 good results.
 In the second half of the thesis, an extension to TOBPTT
 is proposed for dealing with the problem of navigation
 and obstacle avoidance. This method separates the two
 tasks of "navigation" and "control"
 by first using an array of interconnected processors to
 rapidly compute a navigational field associated with the
 current obstacle configuration. The algorithm which
 accomplishes this is called the cumulative barrier
 method. It guarantees the absence of local minima and
 ensures that gradient-induced paths steer clear of
 obstacle surfaces. Then, a multi-layer network is
 trained to steer the vehicle along the negative gradient
 of the resulting field. No retraining is necessary when
 obstacles are moved; rather, the array simply tracks the
 obstacle movements and provides the new gradient
 information to the controller. A variant of the truck-
 backer is used with good results in testing this method.
#####################
* Score: 831.8464223336346
* Doc #: 1958
* Title: PLAN: PYRAMID LEARNING ARCHITECTURE NEUROCOMPUTER
* Autho: PECHANEK, GERALD GEORGE
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; ARTIFICIALINTELLIGENCE
* Abstr: In this research, a set of high performance neurocomputer criteria is
 developed for the emulation of high connectivity neural network models,
 supporting both direct and virtual processing of network functions
 including learning. Based on the criteria, a number of digital
 neurocomputers are advanced, that meet the high performance criteria,
 utilizing simple instruction processing elements for each connection
 weight. The processing elements are incorporated into array structures,
 interconnected by summation and communication trees, in particular folded
 triangular combined function trees, producing a six level hierarchical
 digital machine organization, termed the Pyramid Learning Architecture
 Neurocomputer (PLAN).
      More specifically, Hopfield type networks and back-propagation
 learning on multi-layer networks are reviewed and network properties are
 examined. Based on the reviewed networks, a set of nine attributes are
 advanced as representative of a class of neural networks. From the nine
 attributes a set of six criteria are proposed that a high performance
 neurocomputer should meet. The criteria is translated into a six level
 hierarchical structure, termed PLAN, which is reviewed from a top down
 system perspective. Following the high level PLAN presentation, a detailed
 review of each processing level is conducted, presenting in the following
 order; the structural options considered for Levels five and six, an
 instruction set architecture for Level six, a partitioning theorem and
 algorithms supporting scalability affecting all levels, a virtual
 instruction set architecture for Level six, an instruction set architecture
 for Level four, structural options for Level three, an instruction set
 architecture for Level three, options and examples for Level two, and
 considerations for Level one. Following the detailed system description,
 examples describing the use of PLAN are presented. Finally, a comparison of
 PLAN with other neurocomputer architectures is discussed demonstrating a
 theoretical performance advantage for PLAN Levels four, five, and six as
 compared to the reviewed neurocomputer architectures.
      The important contributions of this research include the development
 of a folded communicating adder tree, digital machine organizations
 utilizing communicating adder trees and folded communicating adder trees
 for neural network emulation, a partitioning theorem and algorithms
 allowing a scalable machine organization, and a hierarchical system
 architecture, PLAN, for the emulation of high connectivity recursive neural
 networks and back-propagation learning on multi-layer networks.
#####################
* Score: 831.8464223336346
* Doc #: 1871
* Title: ACTIVE VISUAL INFERENCE OF SURFACE SHAPE
* Autho: CIPOLLA, ROBERTO
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Robots manipulating and navigating in unmodelled environments need
 robust geometric cues to recover scene structure. Vision can provide some
 of the most powerful cues. However, describing and inferring geometric
 information about arbitrarily curved surfaces from visual cues is a
 difficult problem in computer vision. Existing methods of recovering the
 three-dimensional shape of visible surfaces, e.g. stereo and structure from
 motion, are inadequate in their treatment of curved surfaces, especially
 when surface texture is sparse. They also lack robustness in the presence
 of measurement noise or when their design assumptions are violated. This
 thesis addresses these limitations and shortcomings.
      This thesis develops computational theories relating visual motion
 arising from viewer movements to the differential geometry of visible
 surfaces. It shows how an active monocular observer, making deliberate
 exploratory movements, can recover reliable descriptions of curved surfaces
 by tracking image curves.
      It analyses the deformation of apparent contours (outlines of curved
 surfaces) under viewer motion and shows how surface curvature can be
 inferred from the acceleration of image features. It also considers the
 images of other curves on surfaces, concentrating on aspects of surface
 geometry which can be recovered efficiently and robustly and without the
 requirement of the exact knowledge of viewer motion. An example is the
 recovery of surface orientation and time to contact from the differential
 invariants of the image velocity field computed at image curves.
      An important theme developed in this thesis is that the relative image
 motion of nearby points and curves (expressed as differences of image
 velocities or accelerations (Chapter 2) or the deformation of image curves
 (Chapter 4) and shapes (Chapter 5)) is a reliable cue to surface shape in
 that it is insensitive to the exact details of viewer motion.
      These theories have been implemented and tested using a real-time
 tracking system based on deformable contours (snakes). Examples in which
 the visually derived geometry of piecewise smooth surfaces is used in a
 variety of tasks including the geometric modelling of objects, obstacle
 avoidance and navigation and object manipulation are presented.
#####################
* Score: 831.8464223336346
* Doc #: 1643
* Title: AN APPROACH TO INTERACTIVE CREATIVE REASONING SYSTEMS
* Autho: BLEVIS, ELI BERNARD
* Desc : PSYCHOLOGY, GENERAL; COMPUTER SCIENCE; ARTIFICIALINTELLIGENCE; MUSIC
* Abstr: ISBN:         0-315-61458-7
      Although we cannot claim to know precisely what creativity is, it is
 common to recognize its involvement in a wide range of behaviours. This
 thesis ventures a characterization of creative domains of behaviour as
 domains in which the most available notion of validity is agreement in the
 interaction between agents engaged in a process of that behaviour. We have
 investigated two systems in which creativity plays a role, namely
 depreciation analysis and music composition and analysis.This dissertation
 describes the depreciation analysis system briefly and concentrates on the
 music composition and analysis system as the major example.
      Our investigation has led to a computational model for developing
 systems that can assist in creative tasks. The first step in constructing
 the model has been to recognize that creative behaviours do not necessarily
 involve a truth-based semantics and, consequently, that notions from
 classical truth-based logics may not provide much of a basis for
 interpretation. An alternative semantics for an aspect of creative
 behaviour can be based on the questions "what are the objects involved?",
 "what actions are brought to bear on such objects?", and "by what process
 are new actions introduced?". Postulating answers to these questions can
 form the basis for adapting the computational model to the representation
 of particular behaviours involving creative process. The model also
 requires that we can describe the objects in terms of various qualities of
 focus in order that the computational model of process can be made specific
 to a particular behaviour in an interactive, adaptive manner.
#####################
* Score: 831.8464223336346
* Doc #: 1629
* Title: SOIL LIQUEFACTION POTENTIAL: A KNOWLEDGE-BASED SYSTEM APPROACH
* Autho: LOCKE, GERHARD ERICH
* Desc : ENGINEERING, CIVIL; COMPUTER SCIENCE; ARTIFICIALINTELLIGENCE
* Abstr: Soil liquefaction due to earthquakes has caused considerable damage in
 past seismic events. The ability to predict the potential for soil
 liquefaction of a given site or region has been a major interest of many
 researchers. Methods commonly used today are based on deterministic
 approaches that are not well suited for risk assessment. In addition,
 engineering judgement concerning various uncertainties involved with soil
 liquefaction analysis is typically not directly accounted for in an
 analysis.
      In recent years, knowledge-based expert systems have surfaced as a new
 class of tools that can be used in solving very specific problem domains
 such as soil liquefaction potential. A knowledge-based expert system
 provides the environment by which knowledge concerning soil liquefaction
 can be formalized in a consistent unified approach. Knowledge-based expert
 systems typically incorporate a means for handling uncertainty regarding a
 given domain. They also provide a line of reasoning concerning their
 recommendations. These attributes make them well suited as a tool for
 developing a system to analyze soil liquefaction potential.
      This research effort focused on the development of such a system.
 SOLIPO (SOil LIquefaction POtential) was designed to provide a conditional
 probability of liquefaction and provide the user an indication of the
 confidence level in the analysis performed. The system incorporates five
 statistical models which are based on logistical regression analysis of an
 extensive data base of soil liquefaction case histories. The system selects
 the most appropriate model to use in an analysis, based on user responses
 to system queries. In addition, the system deals with the uncertainties
 associated with model selection, input variables, and methods used to
 obtain certain input variables. The system takes into account the user's
 confidence level regarding input variables. The confidence level is
 propagated using certainty factors.
      A SOLIPO analysis utilizes the same data as an expert would use in
 determining soil liquefaction potential. The built in certainty factors
 reflecting the author's subjective judgement concerning various matters is
 compared to knowledgeable individuals in the domain of soil liquefaction
 analysis.
#####################
* Score: 831.8464223336346
* Doc #: 1397
* Title: CONCEPTUAL GRAPHS IN PICTORIAL DATABASE SYSTEMS
* Autho: MANNION, M. A. G.
* Desc : ENGINEERING, SYSTEM SCIENCE; COMPUTER SCIENCE
* Abstr: The success of database systems in the public and private sectors has
 prompted a desire to extend their facilities and expand their range so as
 to cover pictorial applications. The components of pictorial database
 models proposed in the past are reviewed and attention is focused on the
 choice of data model. The relational model, which has proved most popular,
 is now recognised as not suited to handling pictorial information. In
 addition, as database systems acquire more intelligence data structures
 must simplify the task of inferring information which is not explicitly
 requested or represented. The relational model has several drawbacks from
 this point of view and object-oriented data structures are now assuming
 popularity.
      An object-oriented approach to knowledge representation in pictorial
 database systems can be achieved using Conceptual Graphs in which object
 classes form a lattice and instances of classes can be pictorial frames.
 The structures of Conceptual Graphs support a technique for intelligent
 database inference which uses the type lattice and a set of plausible
 concept definitions to extract background knowledge which may be pertinent
 to the query. Furthermore when information can not be found in the database
 they provide a mechanism for invoking special purpose functions which can
 manipulate the image. The strengths and weaknesses of this technique are
 closely examined.
      To follow up these ideas a Relational Picture Language is described
 which allows a pictorial database of conceptual graphs to be set up and
 subsequently interrogated. Each command of the language is explained and
 illustrated by examples. The implementation was written in FRIL, a new
 artificial intelligence language which supports uncertainty.
#####################
* Score: 831.8464223336346
* Doc #: 1308
* Title: DESIGN CONDITIONS AND WORKSPACE FOR THE GENERALLY-PROPORTIONED 3-HINGE SPHERICAL WRIST FOR ROBOTS
* Autho: TRABIA, MOHAMED BAHAA EL-DEEN
* Desc : ENGINEERING, MECHANICAL; ARTIFICIAL INTELLIGENCE
* Abstr: A study of the manipulator spherical wrist and its plane workspace was
 developed for a tool plane which is generally oriented with respect to the
 axis of the outboard-most wrist joint. The problem was treated for wrist
 joints which can freely rotate and for joints, one or more of which, have a
 limited rotation range. The necessary conditions for determination of the
 boundaries of the wrist orientational workspace were obtained. Conditions
 for the wrist full orientation, where the wrist can position the tool at
 any desired orientation, were presented. The necessary conditions for
 determination of the boundaries of the wrist attitudinal workspace were
 obtained. Different techniques were used to determine the boundaries of the
 subset of the attitudinal workspace within which full rotation of the tool
 plane can be completed without approaching a singularity. The techniques
 used depended on using the determinant of the wrist Jacobian, the maximum
 angular speed of the wrist joints, and the maximum angular acceleration of
 the wrist joints. Comparison of these three techniques was presented
 graphically. Numerical examples on how to apply the different design
 conditions were provided.
#####################
* Score: 831.8464223336346
* Doc #: 1084
* Title: MONITORING AND CONTROL OF MACHINING PROCESSES USING NEURAL NETWORKS
* Autho: CHOI, GI SANG
* Desc : ENGINEERING, MECHANICAL; COMPUTER SCIENCE; ENGINEERING,ELECTRONICS AND ELECTRICAL; ARTIFICIAL INTELLIGENCE
* Abstr: In this dissertation, a study on the acoustic emission (AE) from
 orthogonal metal cutting process is done based on Kannatey-Asibu and
 Dornfeld's work (61) and is further extended to both oblique and three
 dimensional single point metal cutting processes. The study is particularly
 concentrated on providing a theoretical background for the proportionality
 between the energy content of AE and the power dissipated in metal cutting
 process, and on deriving relations for theoretical RMS AE level. For
 orthogonal cutting, a new expression for the RMS AE level which is not a
 function of the chip-tool contact length, and the length of sliding
 friction on the rake face, is developed from Kannatey-Asibu and Dornfeld's
 model (61), and is experimentally evaluated. In the experimental evaluation
 of reasonably good correlation with the theory for the variations in
 cutting speed, or depth of cut was observed.
      In this study, an on-line tool wear detection system for turning
 operations is developed, and experimentally evaluated. The results of
 experimental evaluation show that the system works well over a wide range
 of cutting conditions, and the ability of the system to detect tool wear is
 improved due to the generalization, fault-tolerant and self-organizing
 properties of the neural network. It is also demonstrated that the
 performance and the reliability of tool wear detection can be significantly
 improved by fusing information from force and AE sensors using the neural
 network structure.
      In this dissertation, the feasibility of using an adaptive resonance
 network (ART2) with unsupervised learning capability for tool wear
 detection in turning operations is investigated. The experimental results
 show that tool wear can be effectively detected with or without minimum
 prior training using the self-organization property of the ART2 network.
      The final topic of this dissertation is control of machining process
 using the multilayered perceptron type neural networks. The results of the
 experimental evaluation manifest that the input/output relationship of the
 machining process can be effectively simulated with the neural networks and
 the adaptive optimal control system based on the neural network works
 reasonably well. (Abstract shortened with permission of author.)
#####################
* Score: 689.3608753225844
* Doc #: 526
* Title: MODELING COMPLEX MANUFACTURING PROCESSES VIA INTEGRATION OF INFLUENCE DIAGRAMS AND NEURAL NETWORKS
* Autho: NADI, FARIBORZ
* Desc : ENGINEERING, MECHANICAL; ENGINEERING, ELECTRONICS ANDELECTRICAL; COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: An adaptive learning architecture has been developed for modeling
 manufacturing processes involving several controlling variables. This work
 describes the application of the new architecture to modeling and
 synthesizing recipes for oxide thickness in dry oxidation of silicon, and
 deposition rate, stress, and film thickness in Low Pressure Chemical Vapor
 Deposition (LPCVD) of undoped polysilicon. In this architecture the model
 for a process is generated by combining the qualitative knowledge of human
 experts, captured in the form of influence diagrams, and the learning
 abilities of neural networks for extracting the quantitative knowledge that
 relates parameters of a process. To demonstrate the merits of this
 architecture, we have compared the accuracy of these new models to that of
 more conventional models generated by the use of first principles and/or
 statistical regression analysis. Accuracy of the different models is
 compared using the same empirical data sets from realistic experiments. The
 models generated by the integration of influence diagrams and neural
 networks are shown to have half the error or less, even though given only
 half as much information in creating the models. Furthermore, by employing
 the generalization ability of neural networks in the synthesis algorithm,
 we have shown how this architecture can produce new recipes for the
 process. Two such recipes are generated for the LPCVD process. One is a
 zero-stress polysilicon film recipe; the second, a uniform deposition rate
 recipe with the use of a non-uniform temperature distribution during
 deposition.
#####################
* Score: 689.3608753225844
* Doc #: 288
* Title: THE ACCEPTANCE AND EFFECTIVENESS OF HYPERTEXT SYSTEMS IN LEGAL EDUCATION: AN EXPERIMENTAL EVALUATION
* Autho: TROTTER, DAN LEWIS
* Desc : BUSINESS ADMINISTRATION, MANAGEMENT; ARTIFICIAL INTELLIGENCE; EDUCATION, BUSINESS; EDUCATION, TECHNOLOGY
* Abstr: For years, AI/Expert Systems has been studied for its
 relevance to the legal field, and to the field of legal
 and business education. Hypertext is rooted in AI and
 cognitive science theory and has been proposed to be an
 advanced tool for learning.
 This research seeks two objectives: to study the
 acceptance of hypertext-based systems for legal
 education by the students, and to evaluate the
 effectiveness of such systems. To accomplish the first
 objective, Davis' Technology Acceptance Model (TAM) was
 used to model the factors contributing to the acceptance
 of the system by the students. To evaluate the
 effectiveness of the system, a controlled laboratory
 experiment was conducted.
 A hypertext system named "O&A" has been
 developed by the researcher. The system incorporates
 knowledge from the domain of the offer and acceptance
 area of commercial law. Senior business students
 participated in the study as experimental subjects. Each
 student was randomly assigned to the control group or
 the experimental group. In the latter case, the student
 would use O&A to learn the particular domain for a
 written exercise.
 The results indicate the Technology Acceptance Model
 received general support, but that there is no
 significant difference in performance between the
 control and experimental group. Contrary to
 expectations, five of six hypotheses designed to test
 effectiveness showed no significant advantage of
 hypertext over traditional methods. These results were
 interpreted to have been a product of experimental
 design limitations, in that students had very restricted
 exposure to the system in the learning process.
 Although the results on teaching effectiveness were not
 found to be significant, the path analysis for TAM
 yielded many interesting findings. It can be concluded,
 at least in the context of the current experiment, that
 TAM predicts behavioral intention well, that perceived
 usefulness is not a major determinant of behavioral
 intention, that behavioral intention is affected more by
 attitude than by perceived usefulness, and that attitude
 is more affected by perceived usefulness than by ease of
 use. Future studies may involve the replication of the
 current experiment by increasing the intensity of
 hypertext usage throughout the learning process.
#####################
* Score: 60.733859210560375
* Doc #: 4113
* Title: APPLICATION OF VECTOR-NETWORK THEORY AND ARTIFICIAL INTELLIGENCE TO DYNAMICS
* Autho: WONG, CLEMENT CHEUK-WAH
* Desc : ENGINEERING, MECHANICAL; ARTIFICIAL INTELLIGENCE
* Abstr: The purpose of this research was to develop an
 intelligent algorithm for the formulation and solution
 of problems in planar dynamics, given only a simple
 description of the system as input. The algorithm is
 intended to solve problems involving planar motion, with
 or without closed kinematic loops, which require work-
 energy methods, impulse-momentum methods or derivation
 of equations of motion.
 This research objective was addressed by utilizing a
 combination of Vector-Network Theory and Blackboard
 Architecture. Vector-Network Theory is a "graph-
 theoretic" approach to dynamics. It is useful for
 derivation of equations of motion and it has been
 extended to include work-energy and impulse-momentum
 formulations for planar systems of rigid bodies.
 Blackboard Architecture is a popular problem-solving
 model in Artificial Intelligence. The resulting program
 is called The Dynamics Blackboard System (DBS) and it
 links to Maple for symbolic formulations and
 calculations.
 The DBS graphical user interface (GUI) allows users to
 describe a system in a convenient "point-and-click"
 manner. The GUI then converts the user's description of
 a system into the corresponding vector-network model.
 After performing the vector-network symbolic
 formulation, the DBS blackboard algorithm parses the
 unknown variables specified by the user and organizes a
 set of equations suitable for the solution.
 The capability of DBS in dynamics problem solving is
 demonstrated in this thesis by solving several examples
 which cover a wide range of dynamics problems. The
 achievements made in this research show that the Vector-
 Network Theory is a sound basis for AI applications and
 that the Blackboard Architecture, as implemented in DBS,
 shows great potential for further possibly commercial
 development.
#####################
* Score: 60.733859210560375
* Doc #: 23
* Title: A NEURAL NETWORK APPROACH TO SINGLE AND MULTIDIMENSIONAL MODEL BASED ADAPTIVE CONTROL
* Autho: MEGAN, LAWRENCE
* Desc : ENGINEERING, CHEMICAL; ENGINEERING, SYSTEM SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Stricter environmental regulations and a greater need
 for waste minimization have increased the importance of
 process control to chemical plant operations. However,
 the standard PID controller common to most plants often
 does not offer sufficient performance as the controller
 is not designed to account for the nonlinear or
 nonstationary behavior of most processes. The desire to
 improve controller performance has led to a growing
 interest in adaptive control. Adaptive controllers
 typically operate in a model based fashion, where
 process data is used to update the parameters of a
 simple linear process model so that the model remains
 descriptive of the process dynamic behavior. The
 controller parameters are then related to the model
 parameters in a manner which allows the controller to
 maintain robust performance.
 This research investigates a pattern recognition
 approach to adaptive control. Pattern recognition
 techniques analyze the input and output process response
 following dynamic events such as set point changes and
 significant disturbances. They then translate these
 responses into descriptive pattern features. Past
 pattern recognition techniques have used a rule based or
 expert system approach, where a series of rules
 translate features such as overshoot and damping into
 controller parameter updates. This work uses Artificial
 Neural Networks (ANN) for the pattern analysis task due
 to their inherent ability to analyze the entire pattern
 as opposed to particular features, making them less
 susceptible to measurement noise and other
 irregularities. The ANN's are used to translate dynamic
 input/output responses into a measure of the
 multiplicative mismatch between a present model
 parameter and the value of the model parameter which is
 descriptive of the process. By focusing on parameter
 mismatches as opposed to actual parameter values, the
 adaptive strategies maintain process independence.
 Focusing on model parameters as opposed to controller
 parameters allows the methods to be applied to a number
 of model based control algorithms, including an IMC
 based PID structure and Dynamic Matrix Control. This
 work implements the adaptive techniques on a variety of
 challenging single and multivariable processes in an
 effort to determine the strengths, weaknesses and limits
 of the pattern recognition approach.
#####################
* Score: 60.733859210560375
* Doc #: 1899
* Title: QUERY OPTIMIZATION IN DEDUCTIVE AND RELATIONAL DATABASES
* Autho: MUMICK, INDERPAL SINGH
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: Optimization is critical to the success of declarative database
 systems. We develop a powerful extended magic-sets transformation (EMST)
 for optimization of complex queries in relational and deductive database
 systems. EMST works by rewriting database queries so that predicates are
 applied as early as possible (predicate push down) during a bottom-up
 evaluation of the rewritten query. The magic-sets transformation has
 earlier been proposed as an optimization technique for recursive queries in
 deductive databases. We strengthen the technique into an invaluable
 optimization for all types of queries in practical database systems. (1)
 The magic-sets transformation can only use equality predicates to restrict
 computation. We develop the ground magic-sets transformation to push down
 arbitrary built-in predicates, such as Salary $>$ 70K. (2) The magic-sets
 transformation is not applicable in presence of duplicates and aggregates
 supported by practical query languages such as SQL. We define formal
 semantics for a language with duplicates, aggregates, and recursion, and
 define EMST for such a language. (3) We demonstrate the importance of EMST
 on nonrecursive queries through performance experiments on IBM's DB2
 database system. We compare EMST with correlation, a traditional SQL
 optimization technique for pushing down predicates in nonrecursive queries.
 The conclusion is that EMST is a stable transformation and should replace
 correlation.
      A subset of the extended magic-sets transformation has been
 implemented in the Starburst extensible database system being developed at
 the IBM Almaden Research Center. The extended magic-sets transformation
 furthers the state of the art in query optimization for deductive and
 relational databases.
#####################
* Score: 2463.9402890085184
* Doc #: 2201
* Title: STUDENT CONSTRUCTION OF EXPERT SYSTEMS: IMPACT ON CONTENT AREA LEARNING AND PROBLEM-SOLVING
* Autho: KNOX-QUINN, CAROLYN HARPER
* Desc : EDUCATION, TECHNOLOGY; EDUCATION, TEACHER TRAINING;EDUCATION, CURRICULUM AND INSTRUCTION; ARTIFICIALINTELLIGENCE
* Abstr: The use of student-created expert systems to facilitate the learning
 of facts, heuristics, and procedures in tax accounting was investigated.
 Using case study methodology, this study examined student construction of
 expert systems, and the impact of this instructional experience on content
 area learning and problem-solving behavior.
      Seven MBA students created expert systems to advise whether a single
 income or loss activity was passive. The task required knowledge of a
 narrow segment of tax accounting called passive activity limitations (PAL),
 a section of the Tax Reform Act of 1986, which prohibited applying passive
 losses and credits against active and portfolio income as a tax shelter.
      Before and after creating expert systems, the students took a test
 applying the PAL to a set of problems. Their tape-recorded concurrent
 protocols were transcribed and analyzed to provide two models of problem
 solving for each student: the student's initial knowledge and the student's
 knowledge after creating an expert system. A comparison of these models
 highlighted the extent to which creating an expert system affected student
 knowledge and problem-solving behavior.
      Seven accounting professionals, identified by their colleagues as
 expert in PAL, were asked to solve the same problems as those given the
 students. Their recorded concurrent verbal protocols were analyzed to
 provide a model of the experts' decision-making process in this domain.
 Student performance subsequent to creating an expert system was compared to
 the experts' model to highlight similarities and differences between their
 problem-solving behavior and that of PAL experts.
      The results indicated that students were able to create expert systems
 within an amount of time appropriate for class projects. The students' PAL
 knowledge base increased dramatically as a function of creating expert
 systems, and the quality of student problem solving closely mimicked that
 of real-world experts.
      Results of this study validate student construction of expert systems
 to facilitate problem-solving ability within content areas. Furthermore,
 the results suggest factors important to classroom use of expert system
 construction, including: (a) student familiarity with concepts such as
 program modularity and debugging; and (b) narrowly defined expert system
 domains.
#####################
* Score: 2463.9402890085184
* Doc #: 2419
* Title: EFFICIENT NEURAL NETWORK ALGORITHMS FOR MULTICLASS PROBLEMS
* Autho: ANAND, RANGACHARI
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: One connectionist approach to the classification problem, which has
 gained popularity in recent years, is the use of backpropagation-trained
 feedforward neural networks. Although backpropagation has enjoyed wide
 popularity, it has been observed that the rate of convergence of error is
 very low in many applications.
      For two class problems in which the numbers of exemplars for the two
 classes differ greatly, we show that the low rate of convergence of net
 error occurs because the negative gradient vector computed by
 backpropagation for an imbalanced training set does not initially decrease
 the error for the subordinate class. The subsequent rate of convergence of
 the net error is very low. We suggest a modified technique for calculating
 a direction in weight-space which is downhill for both classes. Using this
 algorithm, we have been able to accelerate the rate of learning for
 two-class classification problems by an order of magnitude.
      For multiclass problems, we show that while backpropagation will
 reduce the Euclidean distance between the actual and desired output
 vectors, the difference between some of the components of these vectors
 will actually increase in the first iteration. Unlike the two-class
 problem, there does not exist a direction which is downhill for all
 classes. We therefore propose a modular network architecture to improve the
 rate of learning for multiclass problems. Our basic approach is to reduce a
 K-class problem to set of K two-class problems with a separately trained
 network for each of the K problems. We find that speedups of about one
 order of magnitude can be obtained with this approach.
      We have also addressed the problem of analyzing images containing
 multiple sparse overlapped patterns. This problem arises naturally when
 analyzing the composition of organic macromolecules using data gathered
 from their NMR spectra. Using a neural network approach, we have achieved
 high correct classification percentages (about 87%) for images containing
 as many as five substantially distorted overlapping patterns. We have
 obtained speedups of an order of magnitude when training networks for this
 problem using our improved training algorithm.
#####################
* Score: 2463.9402890085184
* Doc #: 2439
* Title: OBJECT-ORIENTED MODELING AND INTELLIGENT QUERY PROCESSING IN SPATIAL DATABASES
* Autho: SUBRAMANIAN, RAMESH
* Desc : COMPUTER SCIENCE; ENGINEERING, SYSTEM SCIENCE; ARTIFICIALINTELLIGENCE
* Abstr: Spatial Data Modeling is currently an active area of research. A
 related area that is also currently being actively researched, is the
 development of "analysis operators" or flexible operators ("imprecise
 spatial operators").
      The Spatial Data Models that have been suggested have certain
 limitations. They are ad hoc. No design methodology is followed.
 Researchers recognize the need for representing different perspectives
 within the same schema. However, in actuality, most of the proposed models
 use different schemas to represent different perspectives. There are also
 no facilities available for reasoning about the user perspective, given a
 query, or for dynamically processing imprecise spatial query operators.
 Even though certain imprecise operators (e.g. adjacent, near) have been
 implemented, a limited approach has been taken. The operators are assumed
 to have precise interpretations. The notion of changes in the meanings of
 the operators with respect to the user perspective has not been
 incorporated. Facilities to capture the user's spatial perceptions are not
 provided. No underlying methodology is used in developing processing
 approaches for imprecise spatial operators.
      This research addresses issues related to spatial data modeling, query
 processing strategies for certain semantically imprecise spatial operators,
 and certain complex geographic applications such as districting.
      We develop an Object-oriented Spatial Data Model, that is motivated by
 the Probe Spatial Data Model. The methodology adopted for developing the
 model is the Responsibility-Driven Approach. Our model supports the
 representation of certain naturally occurring relationships (e.g. the
 containment hierarchy). The model is adequate for deriving, on a dynamic
 basis, the many and complex relationships that exist among the objects.
 Furthermore, the model incorporates into one schema various user
 perspectives and different object representations in relation to
 perspective.
      We develop operational algorithms for processing such imprecise
 operators as between, close-to, and adjacent-to, using an Analogical
 Reasoning approach.
      We implement and validate the data model and query processor on the
 LAURE object-oriented system, using real life geographic data provided by
 the Census Bureau. We show how our work can be applied to real life
 districting problems.
#####################
* Score: 2463.9402890085184
* Doc #: 2581
* Title: STATISTICALLY MOTIVATED DEFAULTS
* Autho: GOODWIN, SCOTT D.
* Desc : COMPUTER SCIENCE
* Abstr: ISBN:         0-315-70139-0
      Default reasoning is a fundamental area of research in Artificial
 Intelligence. A default is knowledge that is generally true though it
 admits exceptions (e.g., birds fly, objects retain their colour when moved,
 and people with colds cough). The approach taken here is to view a default
 as a statistical claim about the world plus an applicability criterion. The
 assumed applicability of a default to a particular case is thereby governed
 by the same criteria governing the assumed applicability of statistical
 knowledge to a particular case--the problem of determining default
 applicability is essentially the famous problem of reference class
 selection in probability theory.
      We examine the problem of reference class selection within the context
 of Bacchus and Halpern's combined probability logic and we introduce a new
 approach based on the idea of second order randomization. Under this
 scheme, the assumed independence properties of particular predicates are
 based on the independence properties of randomized predicates. This is a
 natural extension to first order randomization, where the degree of belief
 in a proposition about a particular individual is based on the statistical
 properties of randomized individuals. The flavour is much like that of
 maximum entropy approaches but we avoid problems such as syntax
 sensitivity. This approach accounts for many intuitive default inferences
 and solves problems involving conflicting sources of statistical knowledge
 that present difficulties for many other approaches.
#####################
* Score: 2463.9402890085184
* Doc #: 2783
* Title: TROUBLESHOOTING UNFAMILIAR DEVICES
* Autho: VOLOVIK, DMITRY
* Desc : COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: The dissertation presents a theory of troubleshooting unfamiliar
 devices. The theory explains the troubleshooting processes for unfamiliar
 devices by means of enabling constraints that must be satisfied in
 troubleshooting task environments. Provided these constraints are met, the
 theory derives necessary and sufficient conditions on computations. The
 computations are carried out by agents that efficiently perform the task of
 troubleshooting complex unfamiliar devices in the task environment. The
 dissertation discusses several instances of a troubleshooting process that
 occurs in the absence of device-specific troubleshooting experience in the
 domain of digital electronic devices. In this domain troubleshooters
 develop a number of specific heuristic methods. Failures and limitations of
 these heuristic methods are discussed. The dissertation constructs a
 framework and a formal theory of troubleshooting abstract unfamiliar
 devices. The theory is based on enabling constraints as axioms. The
 dissertation presents an efficient algorithm for troubleshooting unfamiliar
 devices based on the requirements of the theory. The dissertation discusses
 limiting constraints in troubleshooting task environments. The dissertation
 proposes that device-specific knowledge of designers has the power to
 overcome some of these constraints. The dissertation presents a number of
 modifications to the above algorithm. These modifications are based on
 relaxing limiting constraints. To relax limiting constraints the
 dissertation uses various descriptions embedded in natural by-products of
 the device design process, such as components descriptions, test
 descriptions, finite state control descriptions, and hierarchical
 functional device decompositions. Designers create the by-products in the
 process of design and use them to circumscribe the complexity of the design
 and debugging task. The dissertation discusses data from an experiment in
 the domain of digital electronic devices. The experiment demonstrates the
 difference in performance on simulated troubleshooting tasks of several
 algorithms that carry out the troubleshooting process for unfamiliar
 devices in this domain. In particular, the experiment demonstrates that the
 above algorithm outperforms heuristic methods of troubleshooters on complex
 devices. In addition, various algorithmic extensions based on relaxing
 different limiting constraints are compared to identify the effects on
 performance of relaxing individual limiting constraints.
#####################
* Score: 2463.9402890085184
* Doc #: 2880
* Title: AN APPLICATION OF UNSUPERVISED NEURAL NETWORKS AND FUZZY CLUSTERING IN THE IDENTIFICATION OF STRUCTURE IN PERSONALITY DATA
* Autho: LAUDEMAN, IRENE VINCIE
* Desc : PSYCHOLOGY, PERSONALITY; PSYCHOLOGY, PSYCHOMETRICS; ARTIFICIAL INTELLIGENCE
* Abstr: Questions related to the number and definition of broad
 personality types have long been debated in the
 personality literature. The present research applied new
 methods to these old questions and tested the utility of
 these methods in three large samples with a combined
 sample size of over 3,000. Thus, replicability and
 generalizability were emphasized throughout this study,
 so as to move work on personality typologies from the
 theoretical realm to a level that admits to stringent
 empirical tests.
 The method developed in this study addressed typological
 issues raised in previous work, especially the reliable
 identification of the number of distinct types, the
 definition of type membership in nondiscrete, "fuzzy"
 terms, and the identification of nonlinear patterns in
 the data. The method borrowed heavily from techniques
 developed in the discipline of pattern recognition and
 used Big Five scale scores as input variables. The input
 variables were analyzed with both neural network and
 fuzzy clustering implementations of the c-means
 clustering algorithm.
 The method was used to derive a personality typology
 consisting of five broad person types which were
 identified in six data sets that were collected from
 three populations and used two personality inventories.
 The five cluster structure was identified by repeated
 neural network analyses of the six data sets. The
 cluster centers or mean personality profiles for each of
 the five clusters or personality types were found using
 fuzzy clustering analyses. Additionally, a fuzzy
 membership value was computed for each case in each of
 the five clusters. Each case was then assigned to the
 cluster in which it had the highest membership value.
 Of the five personality types identified in this study,
 three were similar to personality types reported in
 previous work. The identification of the two previously
 undetected personality types was attributed to the
 application of a method that used Big Five scales as
 input variables, reliably identified an optimal number
 of clusters and represented the nonlinear patterns in
 the data.
#####################
* Score: 2463.9402890085184
* Doc #: 2883
* Title: THE PARAMETERIZATION OF A FUZZY INFERENCE SYSTEM AND ITS APPLICATION TO CONTROL
* Autho: CHANG, PING-WEI
* Desc : ENGINEERING, MECHANICAL; ARTIFICIAL INTELLIGENCE
* Abstr: Fuzzy Logic Control (FLC) may be viewed as a knowledge-
 based control strategy that is cast in the form of a
 rule base consisting of inference propositions. This
 heuristic approach to control design produces a control
 law that appears drastically different from the
 conventional framework of control design, which takes
 the form of analytical formulation. In order to provide
 a tool for analyzing the heuristic control design, we
 develop a methodology for transforming the fuzzy
 inference process, which is the core of a fuzzy logic
 control system, into an equivalent mapping between the
 antecedent universe and the consequence universe of the
 rule base.
 Differing from the standard method found in many
 applications of fuzzy logic control, our methodology for
 processing fuzzy inference is based on examining
 optimally synthesized possibility distributions. As a
 result, we discover a way of quickly zooming in on the
 optimal consequence of the possibility distribution
 yielded by collective fuzzy implication. Using this
 methodology as a general framework of the fuzzy
 inference system, we develop a numerical scheme to help
 us design a parameterized mapping that optimally
 represents the inference process.
 In transforming a fuzzy inference process to a
 functional mapping, we obtain a useful tool for modeling
 nonlinear control strategies. Based on our empirical
 knowledge of the physical system of interest, we can
 sometimes lay out effective strategies for every
 localized region of the system's operating range. The
 framework of the fuzzy rule base offers a natural tool
 for us to quantify such knowledge. The improved
 analytical property of the equivalent functional mapping
 enables us to easily analyze or modify the fuzzy
 inference system with the aid of conventional techniques
 of control design.
 In order to test the control strategy thus derived, we
 apply our methodology to control the upswing and
 balancing of an inverted pendulum. We demonstrate how we
 transform a fuzzy rule base specifically designed for
 the control of the pendulum upswing into an equivalent
 functional mapping, and how this equivalently
 transformed mapping is modified to improve the overall
 system response.
#####################
* Score: 2463.9402890085184
* Doc #: 3047
* Title: ALGORITHMS FOR COMBINATORIAL OPTIMIZATION IN REAL-TIME AND THEIR AUTOMATED REFINEMENTS BY GENETICS-BASED LEARNING
* Autho: CHU, LON-CHAN
* Desc : COMPUTER SCIENCE; ENGINEERING, ELECTRONICS AND ELECTRICAL; ARTIFICIAL INTELLIGENCE
* Abstr: The goal of this research is to develop a systematic,
 integrated method of designing efficient search
 algorithms that solve optimization problems in real
 time. Search algorithms studied in this thesis comprise
 meta-control and primitive search. The class of
 optimization problems addressed are called combinatorial
 optimization problems, examples of which include many NP-
 hard scheduling and planning problems, and problems in
 operations research and artificial-intelligence
 applications. The problems we have addressed have a well-
 defined problem objective and a finite set of well-
 defined problem constraints. In this research, we use
 state-space trees as problem representations. The
 approach we have undertaken in designing efficient
 search algorithms is an engineering approach and
 consists of two phases: (a) designing generic search
 algorithms, and (b) improving by genetics-based machine
 learning methods parametric heuristics used in the
 search algorithms designed. Our approach is a systematic
 method that integrates domain knowledge, search
 techniques, and automated learning techniques for
 designing better search algorithms. Knowledge captured
 in designing one search algorithm can be carried over
 for designing new ones.
#####################
* Score: 2463.9402890085184
* Doc #: 3326
* Title: A GENERIC SUM OF PRODUCTS PARALLEL PROCESSOR FOR NEURAL NETWORKS AND DIGITAL SIGNAL PROCESSING
* Autho: AIKENS, VALENTINE ST CHRISTOPHER, II
* Desc : ENGINEERING, ELECTRONICS AND ELECTRICAL; COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: In this dissertation a processor which targets
 algorithms using, as the core operation, the sum of
 products calculation is introduced and evaluated. The
 programmable parallel pipelined processor provides
 extremely good performance and flexibility and requires
 a small amount of hardware.
 There are classes of algorithms and applications that
 when implemented on general purpose uniprocessors result
 in poor performance. One class in particular contains
 algorithms which use, as the core operation, the sum of
 products calculation. These algorithms are typically
 applied to large sets of data. Areas in this class
 include artificial neural networks (ANN) learning
 algorithms for ANNs, and digital signal processing
 (DSP).
 A set of computational, communication, and storage
 requirements for general learning in ANNs and digital
 signal processing have been identified. A number of
 diverse algorithms for learning in ANNs and DSP are then
 programmed using the generic sum of products processor
 (GSPP) to show the flexibility of this processor. The
 processor performance for these algorithms has been
 evaluated using a novel evaluation approach as well as a
 simulator of the machine.
 Applications such as NETtalk for ANN learning and 1-K
 discrete Fourier transform (DFT) for digital signal
 processing are used as benchmarks. These benchmarks have
 been used to compare the performance of the proposed
 GSPP to a number of other machines. GSPP performs
 extremely well when compared to other processors that
 require significantly more hardware.
#####################
* Score: 2463.9402890085184
* Doc #: 3328
* Title: EVIDENCE SETS AND CONTEXTUAL GENETIC ALGORITHMS: EXPLORING UNCERTAINTY, CONTEXT, AND EMBODIMENT IN COGNITIVE AND BIOLOGICAL SYSTEMS
* Autho: ROCHA, LUIS MATEUS
* Desc : ENGINEERING, SYSTEM SCIENCE; COMPUTER SCIENCE; ARTIFICIAL INTELLIGENCE
* Abstr: This dissertation proposes a systems-theoretic framework
 to model biological and cognitive systems which requires
 both self-organizing and symbolic dimensions. The
 framework is based on an inclusive interpretation of
 semiotics as a conceptual theory used for the simulation
 of complex systems capable of representing, as well as
 evolving in their environments, with implications for
 Artificial Intelligence and Artificial Life. This
 evolving semiotics is referred to as Selected Self-
 Organization when applied to biological systems, and
 Evolutionary Constructivism when applied to cognitive
 systems. Several formal avenues are pursued to define
 tools necessary to build models under this framework.
 In the Artificial Intelligence camp, Zadeh's Fuzzy Sets
 are extended with the Dempster-Shafer Theory of Evidence
 into a new mathematical structure called Evidence Sets,
 which can capture more efficiently all recognized forms
 of uncertainty in a formalism that explicitly models the
 subjective context dependencies of linguistic
 categories. A belief-based theory of Approximate
 Reasoning is proposed for these structures, as well as
 new insights as to the measurement of uncertainty in
 nondiscrete domains. Evidence sets are then used in the
 development of a relational database architecture useful
 for the data mining of information stored in several
 networked databases. This useful data mining application
 is an example of the semiotic framework put into
 practice and establishes an Artificial Intelligence
 model of Cognitive Categorization with a hybrid
 architecture that possesses both connectionist and
 symbolic attributes.
 In the Artificial Life camp, Holland's Genetic
 Algorithms are extended to a new formalism called
 Contextual Genetic Algorithms which introduces nonlinear
 relationships between genetic descriptions and solutions
 for a particular problem. The nonlinear relationship is
 defined by an indirect scheme based on Fuzzy Sets which
 implements the simulation of dynamic development after
 genetic transcription. Genetic descriptions encode
 dynamic building blocks that self-organize into
 solutions. Since the self-organizing process may depend
 on environmental information, the process is thus
 contextualized. The main advantage of this scheme is the
 ability to reduce dramatically the information
 requirements of genetic descriptions, it also allows the
 transformation of real-encoded to binary-encoded
 problems. The scheme is used successfully to evolve
 Neural Network architectures as well as Cellular
 Automata rules for non-trivial tasks. It is also used to
 model the biological process of RNA Editing. Contextual
 Genetic Algorithms are an instance of the semiotic
 framework proposed and of Selected Self-Organization in
 particular.
#####################
* Score: 2463.9402890085184
* Doc #: 3387
* Title: KNOWLEDGE-BASED APPROACHES TO SELF-ADAPTATION IN CULTURAL ALGORITHMS
* Autho: CHUNG, CHAN-JIN
* Desc : COMPUTER SCIENCE; MATHEMATICS; OPERATIONS RESEARCH; ARTIFICIAL INTELLIGENCE
* Abstr: Cultural Algorithms are computational self-adaptive
 models which consist of a population and a belief space.
 Problem solving experience of individuals selected from
 the population space by the acceptance function is
 generalized and stored in the belief space. This
 knowledge can then control the evolution of the
 population component by means of the influence function.
 Here, we examine the role that different forms of
 knowledge can play in the self-adaptation process for
 evolution-based function optimizers. In particular, we
 compare various approaches using normative and
 situational knowledge in guiding the search process.
 Also we investigate the impact of different acceptance
 and influence functions on the system's performance by
 employing both static and flexible fuzzy approaches.
 Evolutionary Programming is used to implement the
 population space.
 The best performance is produced using knowledge to
 decide both step size and direction in most cases. In
 addition, the use of a fuzzy acceptance and influence
 function appears to be a promising one.
 All the results in this study exhibit that Cultural
 Algorithms are a naturally useful framework for self-
 adaptation and that the use of a cultural framework to
 support self-adaptation in Evolutionary Programming can
 produce substantial performance improvements as
 expressed in terms of (1) system success ratio, (2)
 execution CPU time, and (3) convergence (mean best
 solution) for a given set of function minimization
 problems. The nature of these improvements and the type
 of knowledge that is most effective in producing them
 depends on the structure of the problem. While in most
 cases, the best performance is produced using knowledge
 to decide both step size and direction, there are
 situations where controlling only the direction or the
 step size produces the best results. Also normative
 knowledge appears to be the dominant and general purpose
 knowledge source for the optimization functions here.
#####################
* Score: 2463.9402890085184
* Doc #: 3534
* Title: NON-AXIOMATIC REASONING SYSTEM: EXPLORING THE ESSENCE OF INTELLIGENCE
* Autho: WANG, PEI
* Desc : COMPUTER SCIENCE; PSYCHOLOGY, GENERAL; PHILOSOPHY; ARTIFICIAL INTELLIGENCE; INFORMATION SCIENCE
* Abstr: Every artificial-intelligence research project needs a
 working definition of "intelligence", on which the
 deepest goals and assumptions of the research are based.
 In the project described in the following chapters,
 "intelligence" is defined as the capacity to adapt under
 insufficient knowledge and resources. Concretely, an
 intelligent system should be finite and open, and should
 work in real time.
 If these criteria are used in the design of a reasoning
 system, the result is NARS, a non-axiomatic reasoning
 system.
 NARS uses a term-oriented formal language, characterized
 by the use of subject-predicate sentences. The language
 has an experience-grounded semantics, according to which
 the truth value of a judgment is determined by previous
 experience, and the meaning of a term is determined by
 its relations with other terms. Several different types
 of uncertainty, such as randomness, fuzziness, and
 ignorance, can be represented in the language in a
 single way.
 The inference rules of NARS are based on three
 inheritance relations between terms. With different
 combinations of premises, revision, deduction,
 induction, abduction, exemplification, comparison, and
 analogy can all be carried out in a uniform format, the
 major difference between these types of inference being
 that different functions are used to calculate the truth
 value of the conclusion from the truth values of the
 premises.
 Since it has insufficient space-time resources, the
 system needs to distribute them among its tasks very
 carefully, and to dynamically adjust the distribution as
 the situation changes. This leads to a "controlled
 concurrency" control mechanism, and a "bag-based" memory
 organization.
 A recent implementation of the NARS model, with
 examples, is discussed. The system has many interesting
 properties that are shared by human cognition, but are
 absent from conventional computational models of
 reasoning.
 This research sheds light on several notions in
 artificial intelligence and cognitive science, including
 symbol-grounding, induction, categorization, logic, and
 computation. These are discussed to show the
 implications of the new theory of intelligence.
 Finally, the major results of the research are
 summarized, a preliminary evaluation of the working
 definition of intelligence is given, and the limitations
 and future extensions of the research are discussed.
#####################
* Score: 2463.9402890085184
* Doc #: 3582
* Title: PREDICTING NAVAL AVIATOR FLIGHT TRAINING PERFORMANCE USING MULTIPLE REGRESSION AND AN ARTIFICIAL NEURAL NETWORK
* Autho: GRIFFIN, GLENN RAY
* Desc : EDUCATION, EDUCATIONAL PSYCHOLOGY; EDUCATION, PSYCHOLOGY; EDUCATION, TESTS AND MEASUREMENTS; PSYCHOLOGY, EXPERIMENTAL; EDUCATION, VOCATIONAL; ARTIFICIAL INTELLIGENCE
* Abstr: The Navy needs improved methods for assigning naval
 aviators (pilots) to fixed-wing and rotary-winged
 aircraft. This study evaluated the potential of a series
 of single- and multitask tests to account for additional
 significant variance in the prediction of flight grade
 training performance for a sample of naval aviator
 trainees. Subjects were tested on a series of cognitive
 and perceptual psychomotor tests. The subjects then
 entered the Navy Flight Training Program. Subject's
 flight grades were obtained at the end of primary
 training. Multiple regression and artificial neural
 network procedures were evaluated to determine their
 relative efficiency in the prediction of flight grade
 training performance.
 All single- and multitask test measures evaluated as a
 part of this study were significantly related to the
 primary training flight grade criterion. Two psychomotor
 and one dichotic listening test measures contributed
 significant added variance to a multiple regression
 equation, beyond that of selection tests $F (5,
 428)=27.19, R$ squared =.24, multiple $R=.49, p<.01.$ A
 follow-on analysis indicated a split-half validation
 correlation coefficient of $r=.38, p<.01$ using multiple
 regression and as high as $r=.41, p<.01$ using a neural
 network procedure.
 No statistically significant differences were found
 between the correlation coefficients resulting from the
 application of multiple regression and neural network
 validation procedures. Both procedures predicted the
 flight grade criterion equally well, although the neural
 network applications consistently provided slightly
 higher correlations between actual and predicted flight
 grades.
 The results of this study demonstrated that the single-
 and multitask measures accounted for added unique
 variance beyond that of selection tests in predicting
 flight grades. Since later (intermediate and advanced)
 flight training assignments are determined by flight
 grades earned during the primary portion of training,
 these tests could theoretically be used to predict an
 individual's flight grade and select aviator applicants
 into training pipelines prior to training.
#####################
* Score: 2463.9402890085184
* Doc #: 3625
* Title: LEARNING CONTROL OF A QUADRUPED WALKING MACHINE USING CEREBELLAR MODEL ARTICULATION CONTROLLER NEURAL NETWORKS
* Autho: LIN, YI
* Desc : ENGINEERING, MECHANICAL; APPLIED MECHANICS; ARTIFICIAL INTELLIGENCE
* Abstr: This work aims to explore the power of learning and
 speed of one type of the artificial neural networks,
 namely the Cerebellar Model Articulation Controller
 (CMAC). The capabilities and limitations of CMAC are
 investigated and compared with other types of neural
 networks. Then, a CMAC-based learning algorithm is
 applied to the kinematic control of a walking machine.
 The stability of the CMAC-based control scheme is also
 studied and a mathematical proof of the asymptotical
 stability for the regulation control is derived. The
 developed algorithm is then extended to control both the
 position and force of a two degrees-of-freedom leg
 walking on soft terrain. It is demonstrated that the
 CMAC-based learning system performs better than the mere
 feedback control in terms of speed and accuracy.
 Furthermore, the CMAC-based hybrid force/position
 control is applied to control a quadruped walking
 machine walking on a flat and soft terrain. Finally, the
 proposed walking control are simulated using a self-
 developed animation software package. The entire
 learning control process was accomplished on a PC/AT
 personal computer with a CMAC board.
#####################
* Score: 2463.9402890085184
* Doc #: 3728
* Title: LITTLE LINGUISTIC CREATURES: CLOSED SYSTEMS OF SOLVABLE NEURAL NETWORKS FOR INTEGRATED LINGUISTIC ANALYSIS
* Autho: DROSSAERS, MARC FREDERIC JOOST
* Desc : COMPUTER SCIENCE; LANGUAGE, LINGUISTICS; ARTIFICIAL INTELLIGENCE THE NETHERLANDS
* Abstr: This thesis answers the question for a theory on self-
 organizing neural networks which, if properly organized
 in systems, can be used for integrated linguistic
 analysis, where 'integrated' refers to, among others,
 syntactic, semantic and pragmatic aspects.
 The approach to answering the question is based on the
 ideas of the neural-biologists Maturana and Varela. It
 is directed at machines that unsupervisedly acquire the
 subjective knowledge required for the use of language.
 The thesis presents a class of neural networks that: (1)
 are models of bi-layer modules of the nervous system,
 these modules can be considered its basic functional
 unit; (2) come to perform a task by unsupervised
 learning; (3) are computationally equivalent to finite-
 state acceptors; and (4) can be combined into systems of
 networks, just as the bi-layer modules are combined into
 the nervous system.
 The thesis presents and analyzes: (1) the neural-network
 acceptor, a neural network which is computationally
 equivalent to a finite-state acceptor: the abstract
 machine that is computationally just strong enough for
 natural language processing tasks; (2) the noisy-network
 acceptor, a variant which incorporates the stochastic
 nature of biological neurons in firing action
 potentials; and (3) the parallel-network acceptor, a
 simplified variant with improved computational
 efficiency. The analyses are based on the statistical
 mechanics analyses of the Hopfield model.
 The networks learn to perform functions by means of two
 learning processes. One modifies the synapses within
 layers, and one modifies the synapses between layers. In
 the case of systems of network acceptors, only the
 peripheral layers are given examples to learn from; in a
 system learning proceeds from the periphery inward.
 The functional correctness of systems can be checked by
 first order predicate logic. Because it can be
 determined a priori how individual networks and the
 learning processes will behave, it can also be
 determined a priori how systems of network acceptors
 will behave.
 Since it is in general not known how the brain
 implements language processing tasks it is necessary,
 and very interesting, to construct experimental systems
 of network acceptors for natural language processing:
 little linguistic creatures.
#####################
* Score: 2463.9402890085184
* Doc #: 4023
* Title: A NEURAL NETWORK DECISION MODEL FOR MANAGING PRODUCT MIX IN MANUFACTURING
* Autho: BENHAJLA, SAIDA
* Desc : ENGINEERING, INDUSTRIAL; OPERATIONS RESEARCH; ARTIFICIAL INTELLIGENCE
* Abstr: We address the product mix problem in manufacturing and
 develop optimal and heuristic approaches to solving it
 for realistic product mix problems. Our contribution has
 three important dimensions: the first involves
 developing a neural network to solving the two stage
 stochastic program with mixed integer variables and
 linear or non-linear objective functions. The second
 contribution is the heuristic we design for solving
 multiple criteria optimization problems using an
 integration of our neural network and existing
 interactive multiple objective procedures. Finally, our
 third contribution is to provide computational results
 on how our methodology can be used to make product mix
 decisions that benefit the company as a whole and
 maximize its performance. We use the data collected at a
 major U.S. automaker and we present comparisons of our
 algorithm's computational performance relative to
 existing optimization tools.
 A key advantage to our neural network methodology is
 that we efficiently solve the resulting two stage
 problem (with binary variables). Alternative traditional
 cutting plane methods are not efficient in such cases
 unless the stage II formulation is very simple.
